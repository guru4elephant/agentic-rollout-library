#!/usr/bin/env python3
"""
LLM API Client for agentic-rollout-library.

Provides a unified interface for calling various LLM APIs using OpenAI SDK compatibility.
"""

import asyncio
import logging
import os
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)


class LLMAPIClient:
    """LLM APIå®¢æˆ·ç«¯ - ä½¿ç”¨OpenAI SDKæ¥å£å…¼å®¹å¤šç§LLMæä¾›å•†"""
    
    def __init__(self, api_key: str, base_url: str, model: str, debug: bool = False, max_retries: int = 3,
                 proxy_url: Optional[str] = None):
        """
        Initialize LLM API client.
        
        Args:
            api_key: API key for authentication
            base_url: Base URL of the API endpoint
            model: Model name to use for generation
            debug: Enable debug mode to log all API inputs and outputs
            max_retries: Maximum number of retry attempts on failure (default: 3)
            proxy_url: Optional proxy URL to use for requests (overrides environment variables)
        """
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.debug = debug
        self.max_retries = max_retries
        
        try:
            import openai
            import httpx
            
            # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼ä¼ å…¥çš„ä»£ç†é…ç½®ï¼Œå¦åˆ™æ£€æµ‹ç¯å¢ƒå˜é‡
            if proxy_url:
                http_proxy = proxy_url
                https_proxy = proxy_url
            else:
                # æ£€æµ‹ä»£ç†ç¯å¢ƒå˜é‡
                http_proxy = os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY')
                https_proxy = os.environ.get('https_proxy') or os.environ.get('HTTPS_PROXY')
            
            # å¦‚æœæœ‰ä»£ç†é…ç½®ï¼Œåˆ›å»ºè‡ªå®šä¹‰çš„ httpx å®¢æˆ·ç«¯
            if http_proxy or https_proxy:
                # httpx ä½¿ç”¨ç»Ÿä¸€çš„ä»£ç†é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ https_proxy
                proxy_url = https_proxy or http_proxy
                logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_url}")
                
                # åˆ›å»ºé…ç½®äº†ä»£ç†çš„ httpx å®¢æˆ·ç«¯
                # é‡è¦ï¼štrust_env=False ç¡®ä¿ä¸ä¼šå—ç¯å¢ƒå˜é‡å˜åŒ–å½±å“
                http_client = httpx.AsyncClient(
                    proxy=proxy_url,  # ä½¿ç”¨ proxy å‚æ•°ï¼Œè€Œä¸æ˜¯ proxies
                    timeout=httpx.Timeout(120.0, connect=30.0),  # å¢åŠ è¶…æ—¶æ—¶é—´
                    verify=True,  # éªŒè¯ SSL è¯ä¹¦
                    follow_redirects=True,
                    trust_env=False  # ç¦ç”¨è‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡ï¼Œé¿å…è¢« kodo ç­‰å·¥å…·å¹²æ‰°
                )
                
                # ä½¿ç”¨è‡ªå®šä¹‰ http å®¢æˆ·ç«¯åˆ›å»º OpenAI å®¢æˆ·ç«¯
                self.client = openai.AsyncOpenAI(
                    api_key=api_key,
                    base_url=f"{base_url}",
                    timeout=60.0,
                    http_client=http_client
                )
                logger.info(f"OpenAI å®¢æˆ·ç«¯å·²é…ç½®ä»£ç†è®¿é—®")
            else:
                # æ²¡æœ‰ä»£ç†é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
                self.client = openai.AsyncOpenAI(
                    api_key=api_key,
                    base_url=f"{base_url}",
                    timeout=120.0  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
                )
                logger.debug("OpenAI å®¢æˆ·ç«¯ä½¿ç”¨ç›´è¿æ¨¡å¼ï¼ˆæ— ä»£ç†ï¼‰")
                
        except ImportError as e:
            if "openai" in str(e):
                logger.error("éœ€è¦å®‰è£…openaiåº“: pip install openai")
            elif "httpx" in str(e):
                logger.error("éœ€è¦å®‰è£…httpxåº“: pip install httpx")
            raise
    
    async def generate(self, messages: List[Dict[str, str]], max_tokens: int = 16000, temperature: float = 0.7) -> str:
        """
        Call LLM API to generate response with retry mechanism.
        
        Args:
            messages: List of message dictionaries with 'role' and 'content' keys
            max_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            
        Returns:
            Generated text response
            
        Raises:
            Exception: If all retry attempts fail
        """
        last_exception = None

        if self.debug:
            self._log_debug_request(messages)
        
        for attempt in range(self.max_retries + 1):  # +1 for initial attempt
            try:
                if attempt > 0:
                    # è®¡ç®—é‡è¯•å»¶è¿Ÿ (æŒ‡æ•°é€€é¿: 1, 2, 4ç§’)
                    delay = 2 ** (attempt - 1)
                    logger.warning(f"ğŸ”„ é‡è¯•ç¬¬ {attempt} æ¬¡ï¼Œå»¶è¿Ÿ {delay} ç§’...")
                    await asyncio.sleep(delay)
                
                
                # æ‰§è¡ŒAPIè°ƒç”¨
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                # å¤„ç†ä¸åŒçš„å“åº”æ ¼å¼
                content = self._extract_response_content(response)
                
                # Debugæ¨¡å¼ï¼šè®°å½•APIå“åº”è¯¦æƒ…
                if self.debug:
                    self._log_debug_response(response, content, attempt)
                
                return content
                
            except Exception as e:
                last_exception = e
                error_msg = str(e)
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
                should_retry = self._should_retry_on_error(e, attempt)
                
                if should_retry and attempt < self.max_retries:
                    logger.warning(f"âŒ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries + 1}): {error_msg}")
                    logger.warning(f"   å°†åœ¨å»¶è¿Ÿåé‡è¯•...")
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥æˆ–ä¸åº”é‡è¯•çš„é”™è¯¯
                    if attempt >= self.max_retries:
                        logger.error(f"âŒ APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries + 1})")
                    else:
                        logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼Œé”™è¯¯ç±»å‹ä¸æ”¯æŒé‡è¯•: {error_msg}")
                    
                    raise Exception(f"LLM APIè°ƒç”¨å¤±è´¥ (å°è¯•äº† {attempt + 1} æ¬¡): {error_msg}") from last_exception
        
        # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
        raise Exception(f"LLM APIè°ƒç”¨å¤±è´¥: {str(last_exception)}") from last_exception
    
    def _extract_response_content(self, response) -> str:
        """ä»å“åº”ä¸­æå–å†…å®¹"""
        if hasattr(response, 'choices') and response.choices:
            content = response.choices[0].message.content
            if content is None:
                logger.warning("Response content is None, returning empty string")
                return ""
            return content
        elif isinstance(response, str):
            return response
        elif isinstance(response, dict):
            if 'choices' in response and response['choices']:
                content = response['choices'][0]['message']['content']
                if content is None:
                    logger.warning("Response content is None, returning empty string")
                    return ""
                return content
            elif 'content' in response:
                content = response['content']
                if content is None:
                    logger.warning("Response content is None, returning empty string")
                    return ""
                return content
            elif 'message' in response:
                message = response['message']
                if message is None:
                    logger.warning("Response message is None, returning empty string")
                    return ""
                return message
            else:
                return str(response)
        else:
            return str(response)

    def _log_debug_request(self, request_messages):
        print("Request Messages Begin")
        for m in request_messages:
            print("---------------------")
            print(m)

        print("Request Messages End")
            
    def _log_debug_response(self, response, content: str, attempt: int):
        """è®°å½•è¯¦ç»†çš„å“åº”è°ƒè¯•ä¿¡æ¯"""
        attempt_info = f" (å°è¯• {attempt + 1})" if attempt > 0 else ""
        print(response)

    
    def _should_retry_on_error(self, error: Exception, attempt: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        error_str = str(error).lower()
        
        # ç½‘ç»œç›¸å…³é”™è¯¯ - åº”è¯¥é‡è¯•
        network_errors = [
            'timeout', 'connection', 'network', 'refused', 'unreachable',
            'temporary failure', 'service unavailable', '502', '503', '504'
        ]
        
        # APIé™åˆ¶ç›¸å…³é”™è¯¯ - åº”è¯¥é‡è¯•
        rate_limit_errors = [
            'rate limit', 'too many requests', '429', 'quota exceeded'
        ]
        
        # æœåŠ¡å™¨é”™è¯¯ - åº”è¯¥é‡è¯•  
        server_errors = [
            'internal server error', '500', '502', '503', '504',
            'bad gateway', 'gateway timeout'
        ]
        
        # å®¢æˆ·ç«¯é”™è¯¯ - ä¸åº”è¯¥é‡è¯•
        client_errors = [
            'invalid api key', 'unauthorized', '401', '403', '404',
            'bad request', '400', 'invalid parameter'
        ]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºä¸åº”é‡è¯•çš„é”™è¯¯
        for client_error in client_errors:
            if client_error in error_str:
                logger.debug(f"   ğŸš« å®¢æˆ·ç«¯é”™è¯¯ï¼Œä¸é‡è¯•: {client_error}")
                return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåº”è¯¥é‡è¯•çš„é”™è¯¯
        all_retryable_errors = network_errors + rate_limit_errors + server_errors
        for retryable_error in all_retryable_errors:
            if retryable_error in error_str:
                logger.debug(f"   ğŸ”„ å¯é‡è¯•é”™è¯¯: {retryable_error}")
                return True
        
        # é»˜è®¤æƒ…å†µï¼šå¦‚æœæ˜¯æœªçŸ¥é”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œåˆ™é‡è¯•
        logger.debug(f"   â“ æœªçŸ¥é”™è¯¯ç±»å‹ï¼Œé»˜è®¤é‡è¯•")
        return True
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯è¿æ¥"""
        try:
            if hasattr(self.client, 'aclose'):
                await self.client.aclose()
            elif hasattr(self.client, 'close'):
                await self.client.close()
        except Exception as e:
            logger.warning(f"Error closing client: {e}")


def create_llm_client(api_key: str, base_url: str, model: str, debug: bool = False, max_retries: int = 3,
                     proxy_url: Optional[str] = None) -> LLMAPIClient:
    """
    Factory function to create LLM API client.
    
    Args:
        api_key: API key for authentication
        base_url: Base URL of the API endpoint  
        model: Model name to use for generation
        debug: Enable debug mode to log all API inputs and outputs
        max_retries: Maximum number of retry attempts on failure (default: 3)
        proxy_url: Optional proxy URL to use for requests (overrides environment variables)
        
    Returns:
        Configured LLMAPIClient instance
    """
    return LLMAPIClient(api_key, base_url, model, debug, max_retries, proxy_url)


async def test_llm_connection(client: LLMAPIClient) -> bool:
    """
    Test LLM API connection.
    
    Args:
        client: LLMAPIClient instance to test
        
    Returns:
        True if connection successful, False otherwise
    """
    try:
        test_messages = [
            {"role": "user", "content": "Hello, can you respond with 'API connection successful'?"}
        ]
        
        response = await client.generate(test_messages, max_tokens=50)
        logger.info(f"APIæµ‹è¯•å“åº”: {response}")
        
        if "successful" in response.lower() or "connection" in response.lower():
            logger.info("âœ… APIè¿æ¥æµ‹è¯•æˆåŠŸ!")
            return True
        else:
            logger.warning("âš ï¸ APIè¿æ¥å¯èƒ½æœ‰é—®é¢˜ï¼Œä½†æ”¶åˆ°äº†å“åº”")
            return True
            
    except Exception as e:
        logger.error(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

