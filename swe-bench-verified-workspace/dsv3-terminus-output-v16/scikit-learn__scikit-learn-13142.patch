diff --git a/debug_issue.py b/debug_issue.py
new file mode 100644
index 000000000..6e6b1509a
--- /dev/null
+++ b/debug_issue.py
@@ -0,0 +1,27 @@
+from sklearn.mixture import GaussianMixture
+import numpy as np
+
+# Set random seed for reproducibility
+np.random.seed(42)
+
+X = np.random.randn(1000, 5)
+print('Testing with n_init=5')
+gm = GaussianMixture(n_components=5, n_init=5)
+
+# Fit and get predictions
+gm.fit(X)
+predict_labels = gm.predict(X)
+
+# Now manually check what fit_predict would do
+# First, let's see what the final E-step in fit_predict would return
+_, log_resp_final = gm._e_step(X)
+fit_predict_labels = log_resp_final.argmax(axis=1)
+
+print("Number of mismatches:", np.sum(predict_labels != fit_predict_labels))
+print("Mismatch percentage:", np.mean(predict_labels != fit_predict_labels) * 100, "%")
+
+# Let's also check if the weighted log probabilities are the same
+weighted_log_prob_predict = gm._estimate_weighted_log_prob(X)
+weighted_log_prob_e_step = gm._estimate_weighted_log_prob(X)  # Should be the same
+
+print("Weighted log prob differences:", np.max(np.abs(weighted_log_prob_predict - weighted_log_prob_e_step)))
\ No newline at end of file
diff --git a/debug_order.py b/debug_order.py
new file mode 100644
index 000000000..4c201523d
--- /dev/null
+++ b/debug_order.py
@@ -0,0 +1,17 @@
+from sklearn.mixture import GaussianMixture
+import numpy as np
+
+# Set random seed for reproducibility
+np.random.seed(42)
+
+X = np.random.randn(1000, 5)
+print('Testing order of operations in fit_predict')
+
+gm = GaussianMixture(n_components=5, n_init=5, verbose=1)
+labels = gm.fit_predict(X)
+
+# Now let's see what predict gives us
+predict_labels = gm.predict(X)
+
+print("Number of mismatches:", np.sum(labels != predict_labels))
+print("Mismatch percentage:", np.mean(labels != predict_labels) * 100, "%")
\ No newline at end of file
diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..ad31eb7d5
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,21 @@
+from sklearn.mixture import GaussianMixture
+from sklearn.utils.testing import assert_array_equal
+import numpy as np
+
+# Set random seed for reproducibility
+np.random.seed(42)
+
+X = np.random.randn(1000, 5)
+print('no n_init')
+gm = GaussianMixture(n_components=5)
+c1 = gm.fit_predict(X)
+c2 = gm.predict(X)
+assert_array_equal(c1, c2)
+print('Passed: no n_init')
+
+print('n_init=5')
+gm = GaussianMixture(n_components=5, n_init=5)
+c1 = gm.fit_predict(X)
+c2 = gm.predict(X)
+assert_array_equal(c1, c2)
+print('Passed: n_init=5')
\ No newline at end of file
diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py
index bd34333c0..8920bef18 100644
--- a/sklearn/mixture/base.py
+++ b/sklearn/mixture/base.py
@@ -257,11 +257,6 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):
                 best_params = self._get_parameters()
                 best_n_iter = n_iter
 
-        # Always do a final e-step to guarantee that the labels returned by
-        # fit_predict(X) are always consistent with fit(X).predict(X)
-        # for any value of max_iter and tol (and any random_state).
-        _, log_resp = self._e_step(X)
-
         if not self.converged_:
             warnings.warn('Initialization %d did not converge. '
                           'Try different init parameters, '
@@ -273,6 +268,11 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):
         self.n_iter_ = best_n_iter
         self.lower_bound_ = max_lower_bound
 
+        # Always do a final e-step to guarantee that the labels returned by
+        # fit_predict(X) are always consistent with fit(X).predict(X)
+        # for any value of max_iter and tol (and any random_state).
+        _, log_resp = self._e_step(X)
+
         return log_resp.argmax(axis=1)
 
     def _e_step(self, X):
diff --git a/test_edge_cases.py b/test_edge_cases.py
new file mode 100644
index 000000000..8b6b13aa0
--- /dev/null
+++ b/test_edge_cases.py
@@ -0,0 +1,62 @@
+from sklearn.mixture import GaussianMixture
+from sklearn.utils.testing import assert_array_equal
+import numpy as np
+
+# Test various edge cases
+
+def test_different_n_init_values():
+    """Test with different n_init values"""
+    np.random.seed(42)
+    X = np.random.randn(500, 3)
+    
+    for n_init in [1, 2, 5, 10]:
+        print(f"Testing n_init={n_init}")
+        gm = GaussianMixture(n_components=3, n_init=n_init)
+        c1 = gm.fit_predict(X)
+        c2 = gm.predict(X)
+        assert_array_equal(c1, c2)
+        print(f"Passed: n_init={n_init}")
+
+def test_different_components():
+    """Test with different numbers of components"""
+    np.random.seed(42)
+    X = np.random.randn(500, 3)
+    
+    for n_components in [2, 3, 5]:
+        print(f"Testing n_components={n_components}")
+        gm = GaussianMixture(n_components=n_components, n_init=3)
+        c1 = gm.fit_predict(X)
+        c2 = gm.predict(X)
+        assert_array_equal(c1, c2)
+        print(f"Passed: n_components={n_components}")
+
+def test_warm_start():
+    """Test with warm_start=True"""
+    np.random.seed(42)
+    X = np.random.randn(500, 3)
+    
+    print("Testing warm_start=True")
+    gm = GaussianMixture(n_components=3, n_init=3, warm_start=True)
+    c1 = gm.fit_predict(X)
+    c2 = gm.predict(X)
+    assert_array_equal(c1, c2)
+    print("Passed: warm_start=True")
+
+def test_convergence_warning():
+    """Test case where convergence warning might occur"""
+    np.random.seed(42)
+    X = np.random.randn(100, 3)
+    
+    print("Testing with low max_iter (potential convergence warning)")
+    gm = GaussianMixture(n_components=3, n_init=3, max_iter=1)
+    c1 = gm.fit_predict(X)
+    c2 = gm.predict(X)
+    assert_array_equal(c1, c2)
+    print("Passed: low max_iter")
+
+if __name__ == "__main__":
+    test_different_n_init_values()
+    test_different_components()
+    test_warm_start()
+    test_convergence_warning()
+    print("All edge case tests passed!")
\ No newline at end of file