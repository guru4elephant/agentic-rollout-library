diff --git a/debug_test.py b/debug_test.py
new file mode 100644
index 000000000..595dfbfab
--- /dev/null
+++ b/debug_test.py
@@ -0,0 +1,36 @@
+#!/usr/bin/env python3
+
+import numpy as np
+from sklearn.metrics.cluster import contingency_matrix
+
+# Test case 5: Very large different clusters
+n_samples = 5000000
+labels_true = np.zeros(n_samples)
+labels_pred = np.ones(n_samples)  # All in different cluster
+
+print("Debugging different clusters case:")
+c = contingency_matrix(labels_true, labels_pred, sparse=True)
+print(f"Contingency matrix data: {c.data}")
+print(f"Contingency matrix shape: {c.shape}")
+
+tk = np.dot(c.data, c.data) - n_samples
+pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
+qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
+
+print(f"tk: {tk}")
+print(f"pk: {pk}")
+print(f"qk: {qk}")
+
+# Test the original formula
+try:
+    original = tk / np.sqrt(pk * qk) if tk != 0. else 0.
+    print(f"Original formula result: {original}")
+except Exception as e:
+    print(f"Original formula error: {e}")
+
+# Test the new formula
+try:
+    new = np.sqrt(tk / pk) * np.sqrt(tk / qk) if tk != 0. else 0.
+    print(f"New formula result: {new}")
+except Exception as e:
+    print(f"New formula error: {e}")
\ No newline at end of file
diff --git a/final_test.py b/final_test.py
new file mode 100644
index 000000000..6d3cc4329
--- /dev/null
+++ b/final_test.py
@@ -0,0 +1,69 @@
+#!/usr/bin/env python3
+
+import warnings
+import numpy as np
+from sklearn.metrics.cluster import fowlkes_mallows_score
+
+def test_runtime_warning_fixed():
+    """Test that the RuntimeWarning is no longer triggered"""
+    print("Testing that RuntimeWarning is fixed...")
+    
+    # Capture warnings
+    with warnings.catch_warnings(record=True) as w:
+        warnings.simplefilter("always")
+        
+        # This should have triggered RuntimeWarning before the fix
+        n_samples = 2000000  # 2 million samples
+        labels_true = np.zeros(n_samples)
+        labels_pred = np.zeros(n_samples)
+        
+        score = fowlkes_mallows_score(labels_true, labels_pred)
+        
+        # Check for RuntimeWarnings related to overflow
+        overflow_warnings = [warning for warning in w 
+                           if issubclass(warning.category, RuntimeWarning) 
+                           and 'overflow' in str(warning.message)]
+        
+        if overflow_warnings:
+            print("‚ùå FAILED: Still getting overflow warnings:")
+            for warning in overflow_warnings:
+                print(f"  {warning.message}")
+            return False
+        else:
+            print("‚úÖ SUCCESS: No overflow warnings detected!")
+            print(f"  Score: {score} (expected: 1.0)")
+            return True
+
+def test_nan_fixed():
+    """Test that we no longer get NaN results"""
+    print("\nTesting that NaN results are fixed...")
+    
+    # Test cases that previously returned NaN
+    test_cases = [
+        (2000000, 2000000),  # 2M samples
+        (5000000, 5000000),  # 5M samples
+        (10000000, 10000000), # 10M samples
+    ]
+    
+    for n_samples in [size for size, _ in test_cases]:
+        labels_true = np.zeros(n_samples)
+        labels_pred = np.zeros(n_samples)
+        
+        score = fowlkes_mallows_score(labels_true, labels_pred)
+        
+        if np.isnan(score):
+            print(f"‚ùå FAILED: Still getting NaN for {n_samples} samples")
+            return False
+        else:
+            print(f"‚úÖ SUCCESS: No NaN for {n_samples} samples (score: {score})")
+    
+    return True
+
+if __name__ == "__main__":
+    warning_test = test_runtime_warning_fixed()
+    nan_test = test_nan_fixed()
+    
+    if warning_test and nan_test:
+        print("\nüéâ ALL TESTS PASSED! The fix is working correctly.")
+    else:
+        print("\n‚ùå SOME TESTS FAILED!")
\ No newline at end of file
diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..8f8b9f361
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,46 @@
+#!/usr/bin/env python3
+
+import numpy as np
+from sklearn.metrics.cluster import fowlkes_mallows_score
+
+# Create test data that will cause large pk and qk values
+# We need large cluster sizes to make pk and qk very big
+
+# Test case 1: Very large clusters with many samples
+n_samples = 1000000  # 1 million samples
+labels_true = np.zeros(n_samples)  # All in same cluster
+labels_pred = np.zeros(n_samples)  # All in same cluster
+
+print("Testing with 1 million identical clusters...")
+try:
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"Score: {score}")
+except Exception as e:
+    print(f"Error: {e}")
+
+# Test case 2: Even larger to trigger overflow
+n_samples = 2000000  # 2 million samples
+labels_true = np.zeros(n_samples)
+labels_pred = np.zeros(n_samples)
+
+print("\nTesting with 2 million identical clusters...")
+try:
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"Score: {score}")
+except Exception as e:
+    print(f"Error: {e}")
+
+# Test case 3: Let's also check what happens with the intermediate values
+from sklearn.metrics.cluster import contingency_matrix
+
+print("\nChecking intermediate values:")
+c = contingency_matrix(labels_true, labels_pred, sparse=True)
+tk = np.dot(c.data, c.data) - n_samples
+pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
+qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
+
+print(f"tk: {tk}")
+print(f"pk: {pk}")
+print(f"qk: {qk}")
+print(f"pk * qk: {pk * qk}")
+print(f"np.sqrt(pk * qk): {np.sqrt(pk * qk)}")
\ No newline at end of file
diff --git a/sklearn/metrics/cluster/supervised.py b/sklearn/metrics/cluster/supervised.py
index a987778ae..787aff438 100644
--- a/sklearn/metrics/cluster/supervised.py
+++ b/sklearn/metrics/cluster/supervised.py
@@ -856,7 +856,13 @@ def fowlkes_mallows_score(labels_true, labels_pred, sparse=False):
     tk = np.dot(c.data, c.data) - n_samples
     pk = np.sum(np.asarray(c.sum(axis=0)).ravel() ** 2) - n_samples
     qk = np.sum(np.asarray(c.sum(axis=1)).ravel() ** 2) - n_samples
-    return tk / np.sqrt(pk * qk) if tk != 0. else 0.
+    
+    if tk == 0.:
+        return 0.
+    else:
+        # Use np.sqrt(tk / pk) * np.sqrt(tk / qk) to avoid integer overflow
+        # when pk * qk becomes too large
+        return np.sqrt(tk / pk) * np.sqrt(tk / qk)
 
 
 def entropy(labels):
diff --git a/test_comprehensive.py b/test_comprehensive.py
new file mode 100644
index 000000000..875180edb
--- /dev/null
+++ b/test_comprehensive.py
@@ -0,0 +1,49 @@
+#!/usr/bin/env python3
+
+import numpy as np
+from sklearn.metrics.cluster import fowlkes_mallows_score
+
+def test_edge_cases():
+    print("Testing various edge cases...")
+    
+    # Test 1: Perfect clustering (should be 1.0)
+    labels_true = [0, 0, 1, 1]
+    labels_pred = [0, 0, 1, 1]
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"Perfect clustering: {score} (expected: 1.0)")
+    
+    # Test 2: Reverse clustering (should be 1.0)
+    labels_true = [0, 0, 1, 1]
+    labels_pred = [1, 1, 0, 0]
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"Reverse clustering: {score} (expected: 1.0)")
+    
+    # Test 3: Random clustering (should be 0.0)
+    labels_true = [0, 0, 0, 0]
+    labels_pred = [0, 1, 2, 3]
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"Random clustering: {score} (expected: 0.0)")
+    
+    # Test 4: Large identical clusters
+    n_samples = 5000000  # 5 million samples
+    labels_true = np.zeros(n_samples)
+    labels_pred = np.zeros(n_samples)
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"5M identical clusters: {score} (expected: 1.0)")
+    
+    # Test 5: Very large different clusters
+    n_samples = 5000000
+    labels_true = np.zeros(n_samples)
+    labels_pred = np.ones(n_samples)  # All in different cluster
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"5M different clusters: {score} (expected: 0.0)")
+    
+    # Test 6: Mixed clusters
+    n_samples = 1000000
+    labels_true = np.concatenate([np.zeros(n_samples//2), np.ones(n_samples//2)])
+    labels_pred = np.concatenate([np.zeros(n_samples//2), np.ones(n_samples//2)])
+    score = fowlkes_mallows_score(labels_true, labels_pred)
+    print(f"1M mixed identical clusters: {score} (expected: 1.0)")
+
+if __name__ == "__main__":
+    test_edge_cases()
\ No newline at end of file
diff --git a/test_proper_different.py b/test_proper_different.py
new file mode 100644
index 000000000..f289d4b00
--- /dev/null
+++ b/test_proper_different.py
@@ -0,0 +1,33 @@
+#!/usr/bin/env python3
+
+import numpy as np
+from sklearn.metrics.cluster import fowlkes_mallows_score
+
+# Proper test for different clusters
+# True clusters: [0, 0, 0, 1, 1, 1] 
+# Pred clusters: [0, 1, 2, 0, 1, 2] - completely different
+labels_true = [0, 0, 0, 1, 1, 1]
+labels_pred = [0, 1, 2, 0, 1, 2]
+
+print("Testing proper different clusters:")
+score = fowlkes_mallows_score(labels_true, labels_pred)
+print(f"Score: {score} (expected: 0.0)")
+
+# Now test with large different clusters
+n_samples = 1000000
+# True: first half cluster 0, second half cluster 1
+labels_true = np.concatenate([np.zeros(n_samples//2), np.ones(n_samples//2)])
+# Pred: each sample in its own cluster
+labels_pred = np.arange(n_samples)
+
+print("\nTesting large completely different clusters:")
+score = fowlkes_mallows_score(labels_true, labels_pred)
+print(f"Score: {score} (expected: 0.0)")
+
+# Test edge case where tk is 0
+labels_true = [0, 1, 2, 3]  # Each in own cluster
+labels_pred = [0, 1, 2, 3]  # Each in own cluster
+
+print("\nTesting tk=0 case (all singletons):")
+score = fowlkes_mallows_score(labels_true, labels_pred)
+print(f"Score: {score} (expected: 0.0)")
\ No newline at end of file