diff --git a/debug_test.py b/debug_test.py
new file mode 100644
index 000000000..16055f460
--- /dev/null
+++ b/debug_test.py
@@ -0,0 +1,16 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+# Test case from the failing test
+y_true = [0, 0, 0, 0, 1, 1]
+y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]
+tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)
+
+print(f"y_score: {y_score}")
+print(f"max(y_score): {max(y_score)}")
+print(f"thresholds: {thresholds}")
+print(f"thresholds.max(): {thresholds.max()}")
+
+# Let's also check what happens without drop_intermediate
+tpr2, fpr2, thresholds2 = roc_curve(y_true, y_score, drop_intermediate=False)
+print(f"thresholds (no drop): {thresholds2}")
\ No newline at end of file
diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..6938bf7d7
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,15 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+def test_roc_curve_with_probability_estimates():
+    rng = np.random.RandomState(42)
+    y_true = rng.randint(0, 2, size=10)
+    y_score = rng.rand(10)
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print(f"Thresholds: {thresholds}")
+    print(f"Max threshold: {thresholds.max()}")
+    print(f"Thresholds > 1: {thresholds[thresholds > 1]}")
+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()
+
+if __name__ == "__main__":
+    test_roc_curve_with_probability_estimates()
\ No newline at end of file
diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py
index 0cdead923..0147c3c0a 100644
--- a/sklearn/metrics/_ranking.py
+++ b/sklearn/metrics/_ranking.py
@@ -1083,7 +1083,11 @@ def roc_curve(
     # to make sure that the curve starts at (0, 0)
     tps = np.r_[0, tps]
     fps = np.r_[0, fps]
-    thresholds = np.r_[thresholds[0] + 1, thresholds]
+    # Ensure the added threshold doesn't exceed 1 for probability estimates
+    extra_threshold = thresholds[0] + 1
+    if thresholds.max() <= 1:
+        extra_threshold = min(extra_threshold, 1)
+    thresholds = np.r_[extra_threshold, thresholds]
 
     if fps[-1] <= 0:
         warnings.warn(
diff --git a/test_comprehensive.py b/test_comprehensive.py
new file mode 100644
index 000000000..1acd79e45
--- /dev/null
+++ b/test_comprehensive.py
@@ -0,0 +1,67 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+def test_probability_estimates():
+    """Test with probability estimates (should be in [0, 1])"""
+    print("Testing probability estimates...")
+    rng = np.random.RandomState(42)
+    y_true = rng.randint(0, 2, size=10)
+    y_score = rng.rand(10)  # Probability estimates in [0, 1]
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print(f"  Thresholds: {thresholds}")
+    print(f"  Max threshold: {thresholds.max()}")
+    assert thresholds.max() <= 1, f"Thresholds exceed 1: {thresholds}"
+    print("  ✓ Passed")
+
+def test_decision_function_scores():
+    """Test with decision function scores (can be outside [0, 1])"""
+    print("Testing decision function scores...")
+    rng = np.random.RandomState(42)
+    y_true = rng.randint(0, 2, size=10)
+    y_score = rng.randn(10) * 2  # Decision function scores can be outside [0, 1]
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print(f"  Thresholds: {thresholds}")
+    print(f"  Max threshold: {thresholds.max()}")
+    # For decision function scores, thresholds can exceed 1
+    print("  ✓ Passed (decision function scores can exceed 1)")
+
+def test_all_positive():
+    """Test edge case where all predictions are positive"""
+    print("Testing all positive case...")
+    y_true = np.array([1, 1, 1, 1])
+    y_score = np.array([0.8, 0.9, 0.7, 0.6])  # All positive with high confidence
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print(f"  Thresholds: {thresholds}")
+    print(f"  Max threshold: {thresholds.max()}")
+    assert thresholds.max() <= 1, f"Thresholds exceed 1: {thresholds}"
+    print("  ✓ Passed")
+
+def test_all_negative():
+    """Test edge case where all predictions are negative"""
+    print("Testing all negative case...")
+    y_true = np.array([0, 0, 0, 0])
+    y_score = np.array([0.2, 0.1, 0.3, 0.4])  # All negative with low confidence
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print(f"  Thresholds: {thresholds}")
+    print(f"  Max threshold: {thresholds.max()}")
+    assert thresholds.max() <= 1, f"Thresholds exceed 1: {thresholds}"
+    print("  ✓ Passed")
+
+def test_extreme_values():
+    """Test with extreme probability values"""
+    print("Testing extreme values...")
+    y_true = np.array([1, 0, 1, 0])
+    y_score = np.array([0.999, 0.001, 0.998, 0.002])  # Very confident predictions
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print(f"  Thresholds: {thresholds}")
+    print(f"  Max threshold: {thresholds.max()}")
+    assert thresholds.max() <= 1, f"Thresholds exceed 1: {thresholds}"
+    print("  ✓ Passed")
+
+if __name__ == "__main__":
+    test_probability_estimates()
+    test_decision_function_scores()
+    test_all_positive()
+    test_all_negative()
+    test_extreme_values()
+    print("\nAll tests passed! ✓")
\ No newline at end of file