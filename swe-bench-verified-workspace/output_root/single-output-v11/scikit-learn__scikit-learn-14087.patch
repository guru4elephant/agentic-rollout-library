diff --git a/check_penalty.py b/check_penalty.py
new file mode 100644
index 000000000..7bdb0f056
--- /dev/null
+++ b/check_penalty.py
@@ -0,0 +1,19 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+model = LogisticRegressionCV(
+    cv=5,
+    solver='saga',
+    tol=1e-2,
+    refit=False)
+
+print(f"model.penalty: {model.penalty}")
+print(f"model.l1_ratios: {model.l1_ratios}")
\ No newline at end of file
diff --git a/debug_detailed.py b/debug_detailed.py
new file mode 100644
index 000000000..8e7f73ad7
--- /dev/null
+++ b/debug_detailed.py
@@ -0,0 +1,53 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+# Monkey patch to debug the actual shape at the point of error
+import sklearn.linear_model.logistic as logistic_module
+
+original_fit = logistic_module.LogisticRegressionCV.fit
+
+def debug_fit(self, X, y, sample_weight=None):
+    result = original_fit(self, X, y, sample_weight)
+    return result
+
+# Add debug code to the specific line where error occurs
+def debug_coefs_paths_access(coefs_paths, i, best_indices, folds):
+    print(f"coefs_paths shape: {coefs_paths.shape}")
+    print(f"i: {i}, best_indices[i]: {best_indices[i]}")
+    print(f"folds length: {len(folds)}")
+    try:
+        result = coefs_paths[:, i, best_indices[i], :]
+        print(f"Successfully indexed: {result.shape}")
+        return result
+    except Exception as e:
+        print(f"Error during indexing: {e}")
+        raise
+
+# Replace the problematic line
+import inspect
+lines = inspect.getsource(logistic_module.LogisticRegressionCV.fit)
+print("Original fit method source:")
+print(lines)
+
+logistic_module.LogisticRegressionCV.fit = debug_fit
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+print("Testing LogisticRegressionCV with refit=False...")
+try:
+    model = LogisticRegressionCV(
+        cv=5,
+        solver='saga',
+        tol=1e-2,
+        refit=False).fit(X, y)
+    print("Success! No error thrown.")
+except Exception as e:
+    print(f"Error occurred: {type(e).__name__}: {e}")
+    import traceback
+    traceback.print_exc()
\ No newline at end of file
diff --git a/debug_dimensions.py b/debug_dimensions.py
new file mode 100644
index 000000000..0daec50f7
--- /dev/null
+++ b/debug_dimensions.py
@@ -0,0 +1,54 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+# Monkey patch to add debug info
+import sklearn.linear_model.logistic as logistic_module
+
+original_fit = logistic_module.LogisticRegressionCV.fit
+
+def debug_fit(self, X, y, sample_weight=None):
+    # Call the original fit but catch the error and debug
+    try:
+        result = original_fit(self, X, y, sample_weight)
+        return result
+    except Exception as e:
+        if "too many indices for array" in str(e):
+            # Let's check the shapes at the point of error
+            print("Debugging shapes at error point:")
+            print(f"self.multi_class: {self.multi_class}")
+            print(f"self.refit: {self.refit}")
+            
+            # Check coefs_paths shape
+            if hasattr(self, 'coefs_paths_'):
+                for cls, path in self.coefs_paths_.items():
+                    print(f"coefs_paths[{cls}].shape: {path.shape}")
+            
+            # Check scores shape
+            if hasattr(self, 'scores_'):
+                for cls, score in self.scores_.items():
+                    print(f"scores[{cls}].shape: {score.shape}")
+        
+        raise
+
+logistic_module.LogisticRegressionCV.fit = debug_fit
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+print("Testing LogisticRegressionCV with refit=False...")
+try:
+    model = LogisticRegressionCV(
+        cv=5,
+        solver='saga',
+        tol=1e-2,
+        refit=False).fit(X, y)
+    print("Success! No error thrown.")
+except Exception as e:
+    print(f"Error occurred: {type(e).__name__}: {e}")
+    import traceback
+    traceback.print_exc()
\ No newline at end of file
diff --git a/debug_l1_ratios.py b/debug_l1_ratios.py
new file mode 100644
index 000000000..91b0f80e2
--- /dev/null
+++ b/debug_l1_ratios.py
@@ -0,0 +1,50 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+# Monkey patch to debug the l1_ratios issue
+import sklearn.linear_model.logistic as logistic_module
+
+original_fit = logistic_module.LogisticRegressionCV.fit
+
+def debug_fit(self, X, y, sample_weight=None):
+    try:
+        result = original_fit(self, X, y, sample_weight)
+        return result
+    except Exception as e:
+        if "only integer scalar arrays" in str(e):
+            print("Debugging l1_ratios issue:")
+            print(f"type(best_indices_l1): {type(self.best_indices_l1)}")
+            print(f"best_indices_l1: {self.best_indices_l1}")
+            print(f"type(l1_ratios_): {type(self.l1_ratios_)}")
+            print(f"l1_ratios_: {self.l1_ratios_}")
+            if hasattr(self, 'best_indices_l1') and hasattr(self, 'l1_ratios_'):
+                try:
+                    result = self.l1_ratios_[self.best_indices_l1]
+                    print(f"l1_ratios_[best_indices_l1]: {result}")
+                    print(f"type(result): {type(result)}")
+                except Exception as e2:
+                    print(f"Error in l1_ratios_[best_indices_l1]: {e2}")
+        raise
+
+logistic_module.LogisticRegressionCV.fit = debug_fit
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+print("Testing LogisticRegressionCV with refit=False...")
+try:
+    model = LogisticRegressionCV(
+        cv=5,
+        solver='saga',
+        tol=1e-2,
+        refit=False).fit(X, y)
+    print("Success! No error thrown.")
+except Exception as e:
+    print(f"Error occurred: {type(e).__name__}: {e}")
+    import traceback
+    traceback.print_exc()
\ No newline at end of file
diff --git a/debug_shape.py b/debug_shape.py
new file mode 100644
index 000000000..c61d70db7
--- /dev/null
+++ b/debug_shape.py
@@ -0,0 +1,36 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+print("Testing LogisticRegressionCV with refit=False...")
+try:
+    # Create model but don't fit yet
+    model = LogisticRegressionCV(
+        cv=5,
+        solver='saga',
+        tol=1e-2,
+        refit=False)
+    
+    # Monkey patch to debug the shape
+    original_fit = model.fit
+    def debug_fit(X, y, sample_weight=None):
+        result = original_fit(X, y, sample_weight)
+        print(f"coefs_paths shape: {model.coefs_paths_}")
+        for cls, path in model.coefs_paths_.items():
+            print(f"Class {cls}: shape {path.shape}")
+        return result
+    
+    model.fit = debug_fit
+    model.fit(X, y)
+    print("Success! No error thrown.")
+except Exception as e:
+    print(f"Error occurred: {type(e).__name__}: {e}")
+    import traceback
+    traceback.print_exc()
\ No newline at end of file
diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..827d33780
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,23 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+print("Testing LogisticRegressionCV with refit=False...")
+try:
+    model = LogisticRegressionCV(
+        cv=5,
+        solver='saga', # same error with 'liblinear'
+        tol=1e-2,
+        refit=False).fit(X, y)
+    print("Success! No error thrown.")
+except Exception as e:
+    print(f"Error occurred: {type(e).__name__}: {e}")
+    import traceback
+    traceback.print_exc()
\ No newline at end of file
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 1999f1dae..d0b71f5e3 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -1968,7 +1968,7 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
                 raise ValueError("l1_ratios must be a list of numbers between "
                                  "0 and 1; got (l1_ratios=%r)" %
                                  self.l1_ratios)
-            l1_ratios_ = self.l1_ratios
+            l1_ratios_ = np.asarray(self.l1_ratios)
         else:
             if self.l1_ratios is not None:
                 warnings.warn("l1_ratios parameter is only used when penalty "
@@ -2174,14 +2174,28 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,
                     w = np.mean([coefs_paths[i, best_indices[i], :]
                                  for i in range(len(folds))], axis=0)
                 else:
-                    w = np.mean([coefs_paths[:, i, best_indices[i], :]
-                                 for i in range(len(folds))], axis=0)
+                    if coefs_paths.ndim == 4:
+                        w = np.mean([coefs_paths[:, i, best_indices[i], :]
+                                     for i in range(len(folds))], axis=0)
+                    else:
+                        # Handle case where n_classes=1 and coefs_paths is 3D
+                        w = np.mean([coefs_paths[i, best_indices[i], :]
+                                     for i in range(len(folds))], axis=0)
 
                 best_indices_C = best_indices % len(self.Cs_)
                 self.C_.append(np.mean(self.Cs_[best_indices_C]))
 
                 best_indices_l1 = best_indices // len(self.Cs_)
-                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))
+                
+                # Handle both scalar and array cases for best_indices_l1
+                if np.isscalar(best_indices_l1):
+                    self.l1_ratio_.append(l1_ratios_[best_indices_l1])
+                else:
+                    # Handle case where l1_ratios_ contains None values
+                    if None in l1_ratios_:
+                        self.l1_ratio_.append(None)
+                    else:
+                        self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))
 
             if multi_class == 'multinomial':
                 self.C_ = np.tile(self.C_, n_classes)
diff --git a/test_array_indexing.py b/test_array_indexing.py
new file mode 100644
index 000000000..795a03b98
--- /dev/null
+++ b/test_array_indexing.py
@@ -0,0 +1,16 @@
+import numpy as np
+
+# Test array indexing behavior
+l1_ratios_ = np.array([0.1, 0.5, 0.9])
+best_indices_l1 = np.array([0, 1, 2])  # This is what we get from best_indices // len(self.Cs_)
+
+print(f"l1_ratios_: {l1_ratios_}")
+print(f"best_indices_l1: {best_indices_l1}")
+print(f"l1_ratios_[best_indices_l1]: {l1_ratios_[best_indices_l1]}")
+
+# Test scalar case
+best_indices_l1_scalar = 1
+print(f"l1_ratios_[best_indices_l1_scalar]: {l1_ratios_[best_indices_l1_scalar]}")
+
+# Test what happens with np.mean
+print(f"np.mean(l1_ratios_[best_indices_l1]): {np.mean(l1_ratios_[best_indices_l1])}")
\ No newline at end of file
diff --git a/test_edge_cases.py b/test_edge_cases.py
new file mode 100644
index 000000000..65566798b
--- /dev/null
+++ b/test_edge_cases.py
@@ -0,0 +1,33 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+def test_case(name, **kwargs):
+    print(f"\nTesting {name}...")
+    np.random.seed(29)
+    X = np.random.normal(size=(1000, 3))
+    beta = np.random.normal(size=3)
+    intercept = np.random.normal(size=None)
+    y = np.sign(intercept + X @ beta)
+    
+    try:
+        model = LogisticRegressionCV(refit=False, **kwargs).fit(X, y)
+        print(f"Success! No error thrown for {name}")
+        return True
+    except Exception as e:
+        print(f"Error occurred for {name}: {type(e).__name__}: {e}")
+        return False
+
+# Test different solvers
+test_case("saga solver", solver='saga', tol=1e-2)
+test_case("liblinear solver", solver='liblinear', tol=1e-2)
+
+# Test with elasticnet penalty (should use l1_ratios)
+test_case("elasticnet penalty", penalty='elasticnet', solver='saga', l1_ratios=[0.1, 0.5, 0.9], tol=1e-2)
+
+# Test with different multi_class options
+test_case("multinomial", multi_class='multinomial', solver='saga', tol=1e-2)
+test_case("ovr", multi_class='ovr', solver='saga', tol=1e-2)
+
+print("\nAll tests completed!")
\ No newline at end of file
diff --git a/test_refit_true.py b/test_refit_true.py
new file mode 100644
index 000000000..39c7335cf
--- /dev/null
+++ b/test_refit_true.py
@@ -0,0 +1,25 @@
+import sys
+import sklearn
+from sklearn.linear_model import LogisticRegressionCV
+import numpy as np
+
+np.random.seed(29)
+X = np.random.normal(size=(1000, 3))
+beta = np.random.normal(size=3)
+intercept = np.random.normal(size=None)
+y = np.sign(intercept + X @ beta)
+
+print("Testing LogisticRegressionCV with refit=True...")
+try:
+    model = LogisticRegressionCV(
+        cv=5,
+        solver='saga',
+        tol=1e-2,
+        refit=True).fit(X, y)
+    print("Success! No error thrown with refit=True.")
+    print(f"Model coefficients: {model.coef_}")
+    print(f"Model intercept: {model.intercept_}")
+except Exception as e:
+    print(f"Error occurred: {type(e).__name__}: {e}")
+    import traceback
+    traceback.print_exc()
\ No newline at end of file