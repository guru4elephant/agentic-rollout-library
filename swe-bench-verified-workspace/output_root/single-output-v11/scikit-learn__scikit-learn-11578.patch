diff --git a/final_test.py b/final_test.py
new file mode 100644
index 000000000..c4bb04c44
--- /dev/null
+++ b/final_test.py
@@ -0,0 +1,64 @@
+import numpy as np
+from sklearn import preprocessing, linear_model
+
+# Test the main issue: multinomial scoring should use multinomial probabilities
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b', 'c'], size=samples)  # Multiclass
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+print("=== Testing the main issue: multinomial scoring ===")
+
+# Test with multinomial
+coefs_multinomial, _, scores_multinomial, _ = linear_model.logistic._log_reg_scoring_path(
+    x, y, train, test, fit_intercept=True, scoring='neg_log_loss', multi_class='multinomial'
+)
+
+c_index = 0
+coefs_multinomial = coefs_multinomial[c_index]
+score_multinomial = scores_multinomial[c_index]
+
+# Create LogisticRegression objects with different multi_class settings
+# but same coefficients to verify they produce different probabilities
+
+# Multinomial version
+log_reg_multi = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')
+log_reg_multi.coef_ = coefs_multinomial[:, :-1]
+log_reg_multi.intercept_ = coefs_multinomial[:, -1]
+log_reg_multi.classes_ = np.unique(y[train])
+
+# OvR version  
+log_reg_ovr = linear_model.LogisticRegression(fit_intercept=True, multi_class='ovr')
+log_reg_ovr.coef_ = coefs_multinomial[:, :-1]
+log_reg_ovr.intercept_ = coefs_multinomial[:, -1]
+log_reg_ovr.classes_ = np.array([-1, 1])
+
+# Get probabilities from both
+probs_multi = log_reg_multi.predict_proba(x[test])
+probs_ovr = log_reg_ovr.predict_proba(x[test])
+
+# Binarize labels for scoring
+lb = preprocessing.label.LabelBinarizer()
+lb.fit(y[test])
+y_bin = lb.transform(y[test])
+
+# Calculate scores manually
+manual_score_multi = (y_bin * np.log(probs_multi)).sum(axis=1).mean()
+manual_score_ovr = (y_bin * np.log(probs_ovr)).sum(axis=1).mean()
+
+print(f"Score from _log_reg_scoring_path: {score_multinomial}")
+print(f"Manual multinomial score: {manual_score_multi}")
+print(f"Manual OvR score: {manual_score_ovr}")
+print(f"Multinomial scores match: {abs(score_multinomial - manual_score_multi) < 1e-10}")
+print(f"Multinomial and OvR scores are different: {abs(manual_score_multi - manual_score_ovr) > 1e-10}")
+
+# The key test: the score from _log_reg_scoring_path should match multinomial, not OvR
+print(f"Main issue fixed (score uses multinomial): {abs(score_multinomial - manual_score_multi) < 1e-10}")
+print(f"Main issue fixed (score not using OvR): {abs(score_multinomial - manual_score_ovr) > 1e-10}")
\ No newline at end of file
diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..f4eadf291
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,110 @@
+import numpy as np
+from sklearn import preprocessing, linear_model, utils
+
+def ovr_approach(decision_function):
+    
+    probs = 1. / (1. + np.exp(-decision_function))
+    probs = probs / probs.sum(axis=1).reshape((probs.shape[0], -1))
+    
+    return probs
+
+def score_from_probs(probs, y_bin):
+    
+    return (y_bin*np.log(probs)).sum(axis=1).mean()
+    
+    
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+# Use a "probabilistic" scorer
+scorer = 'neg_log_loss'
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b', 'c'], size=samples)
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+# Binarize the labels for y[test]
+lb = preprocessing.label.LabelBinarizer()
+lb.fit(y[test])
+y_bin = lb.transform(y[test])
+
+# What does _log_reg_scoring_path give us for the score?
+coefs, _, scores, _ = linear_model.logistic._log_reg_scoring_path(x, y, train, test, fit_intercept=True, scoring=scorer, multi_class='multinomial')
+
+# Choose a single C to look at, for simplicity
+c_index = 0
+coefs = coefs[c_index]
+scores = scores[c_index]
+
+# Initialise a LogisticRegression() instance, as in 
+# https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/logistic.py#L922
+existing_log_reg = linear_model.LogisticRegression(fit_intercept=True)
+existing_log_reg.coef_      = coefs[:, :-1]
+existing_log_reg.intercept_ = coefs[:, -1]
+
+existing_dec_fn = existing_log_reg.decision_function(x[test])
+
+existing_probs_builtin = existing_log_reg.predict_proba(x[test])
+
+# OvR approach
+existing_probs_ovr = ovr_approach(existing_dec_fn)
+
+# multinomial approach
+existing_probs_multi = utils.extmath.softmax(existing_dec_fn)
+
+# If we initialise our LogisticRegression() instance, with multi_class='multinomial'
+new_log_reg = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')
+new_log_reg.coef_      = coefs[:, :-1]
+new_log_reg.intercept_ = coefs[:, -1]
+
+new_dec_fn = new_log_reg.decision_function(x[test])
+
+new_probs_builtin = new_log_reg.predict_proba(x[test])
+
+# OvR approach
+new_probs_ovr = ovr_approach(new_dec_fn)
+
+# multinomial approach
+new_probs_multi = utils.extmath.softmax(new_dec_fn)
+
+print('score returned by _log_reg_scoring_path')
+print(scores)
+# -1.10566998
+
+print('OvR LR decision function == multinomial LR decision function?')
+print((existing_dec_fn == new_dec_fn).all())
+# True
+
+print('score calculated via OvR method (either decision function)')
+print(score_from_probs(existing_probs_ovr, y_bin))
+# -1.10566997908
+
+print('score calculated via multinomial method (either decision function)')
+print(score_from_probs(existing_probs_multi, y_bin))
+# -1.11426297223
+
+print('probs predicted by existing_log_reg.predict_proba() == probs generated via the OvR approach?')
+print((existing_probs_builtin == existing_probs_ovr).all())
+# True
+
+print('probs predicted by existing_log_reg.predict_proba() == probs generated via the multinomial approach?')
+print((existing_probs_builtin == existing_probs_multi).any())
+# False
+
+print('probs predicted by new_log_reg.predict_proba() == probs generated via the OvR approach?')
+print((new_probs_builtin == new_probs_ovr).all())
+# False
+
+print('probs predicted by new_log_reg.predict_proba() == probs generated2 via the multinomial approach?')
+print((new_probs_builtin == new_probs_multi).any())
+# True
+
+# So even though multi_class='multinomial' was specified in _log_reg_scoring_path(), 
+# the score it returned was the score calculated via OvR, not multinomial.
+# We can see that log_reg.predict_proba() returns the OvR predicted probabilities,
+# not the multinomial predicted probabilities.
\ No newline at end of file
diff --git a/simple_test.py b/simple_test.py
new file mode 100644
index 000000000..be05d1a44
--- /dev/null
+++ b/simple_test.py
@@ -0,0 +1,55 @@
+import numpy as np
+from sklearn import preprocessing, linear_model
+
+# Test that the fix works for multinomial case
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b', 'c'], size=samples)
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+# Test multinomial case
+print("=== Testing multinomial case ===")
+coefs_multinomial, _, scores_multinomial, _ = linear_model.logistic._log_reg_scoring_path(
+    x, y, train, test, fit_intercept=True, scoring='neg_log_loss', multi_class='multinomial'
+)
+
+c_index = 0
+coefs_multinomial = coefs_multinomial[c_index]
+scores_multinomial = scores_multinomial[c_index]
+
+# Create a LogisticRegression with multinomial to verify the score matches
+log_reg_multinomial = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')
+log_reg_multinomial.coef_ = coefs_multinomial[:, :-1]
+log_reg_multinomial.intercept_ = coefs_multinomial[:, -1]
+log_reg_multinomial.classes_ = np.unique(y[train])
+
+# Binarize the labels for y[test]
+lb = preprocessing.label.LabelBinarizer()
+lb.fit(y[test])
+y_bin = lb.transform(y[test])
+
+probs_multinomial = log_reg_multinomial.predict_proba(x[test])
+manual_score_multinomial = (y_bin*np.log(probs_multinomial)).sum(axis=1).mean()
+
+print(f"Score from _log_reg_scoring_path: {scores_multinomial}")
+print(f"Manual calculation score: {manual_score_multinomial}")
+print(f"Scores match: {abs(scores_multinomial - manual_score_multinomial) < 1e-10}")
+
+# Test that the old behavior (OvR) would have been different
+log_reg_ovr = linear_model.LogisticRegression(fit_intercept=True, multi_class='ovr')
+log_reg_ovr.coef_ = coefs_multinomial[:, :-1]
+log_reg_ovr.intercept_ = coefs_multinomial[:, -1]
+log_reg_ovr.classes_ = np.array([-1, 1])
+
+probs_ovr = log_reg_ovr.predict_proba(x[test])
+manual_score_ovr = (y_bin*np.log(probs_ovr)).sum(axis=1).mean()
+
+print(f"OvR score (for comparison): {manual_score_ovr}")
+print(f"Multinomial and OvR scores are different: {abs(scores_multinomial - manual_score_ovr) > 1e-10}")
\ No newline at end of file
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index e4ea696ce..1c5a42cd0 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -922,7 +922,13 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
 
-    log_reg = LogisticRegression(fit_intercept=fit_intercept)
+    log_reg = LogisticRegression(fit_intercept=fit_intercept,
+                             multi_class=multi_class, solver=solver,
+                             penalty=penalty, dual=dual,
+                             intercept_scaling=intercept_scaling,
+                             class_weight=class_weight,
+                             random_state=random_state, max_iter=max_iter,
+                             tol=tol, verbose=verbose)
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
diff --git a/test_both_cases.py b/test_both_cases.py
new file mode 100644
index 000000000..850c755cc
--- /dev/null
+++ b/test_both_cases.py
@@ -0,0 +1,80 @@
+import numpy as np
+from sklearn import preprocessing, linear_model, utils
+
+def ovr_approach(decision_function):
+    probs = 1. / (1. + np.exp(-decision_function))
+    probs = probs / probs.sum(axis=1).reshape((probs.shape[0], -1))
+    return probs
+
+def score_from_probs(probs, y_bin):
+    return (y_bin*np.log(probs)).sum(axis=1).mean()
+    
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b', 'c'], size=samples)
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+# Binarize the labels for y[test]
+lb = preprocessing.label.LabelBinarizer()
+lb.fit(y[test])
+y_bin = lb.transform(y[test])
+
+# Test multinomial case
+print("=== Testing multinomial case ===")
+coefs_multinomial, _, scores_multinomial, _ = linear_model.logistic._log_reg_scoring_path(
+    x, y, train, test, fit_intercept=True, scoring='neg_log_loss', multi_class='multinomial'
+)
+
+c_index = 0
+coefs_multinomial = coefs_multinomial[c_index]
+scores_multinomial = scores_multinomial[c_index]
+
+# Test OvR case - need to specify pos_class for multiclass
+print("=== Testing OvR case ===")
+# For OvR with multiclass, we need to specify a positive class
+coefs_ovr, _, scores_ovr, _ = linear_model.logistic._log_reg_scoring_path(
+    x, y, train, test, fit_intercept=True, scoring='neg_log_loss', multi_class='ovr', pos_class='a'
+)
+
+coefs_ovr = coefs_ovr[c_index]
+scores_ovr = scores_ovr[c_index]
+
+print(f"Multinomial score: {scores_multinomial}")
+print(f"OvR score: {scores_ovr}")
+
+# Verify they are different (as expected)
+print(f"Scores are different: {abs(scores_multinomial - scores_ovr) > 1e-10}")
+
+# Test with manual calculation for multinomial
+log_reg_multinomial = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')
+log_reg_multinomial.coef_ = coefs_multinomial[:, :-1]
+log_reg_multinomial.intercept_ = coefs_multinomial[:, -1]
+log_reg_multinomial.classes_ = np.unique(y[train])
+
+probs_multinomial = log_reg_multinomial.predict_proba(x[test])
+manual_score_multinomial = score_from_probs(probs_multinomial, y_bin)
+
+print(f"Multinomial score matches manual calculation: {abs(scores_multinomial - manual_score_multinomial) < 1e-10}")
+
+# Test with manual calculation for OvR
+log_reg_ovr = linear_model.LogisticRegression(fit_intercept=True, multi_class='ovr')
+# For OvR with pos_class, coefs_ovr is 1D array
+if coefs_ovr.ndim == 1:
+    log_reg_ovr.coef_ = coefs_ovr[:-1].reshape(1, -1)
+    log_reg_ovr.intercept_ = coefs_ovr[-1:]
+else:
+    log_reg_ovr.coef_ = coefs_ovr[:, :-1]
+    log_reg_ovr.intercept_ = coefs_ovr[:, -1]
+log_reg_ovr.classes_ = np.array([-1, 1])
+
+probs_ovr = log_reg_ovr.predict_proba(x[test])
+manual_score_ovr = score_from_probs(probs_ovr, y_bin)
+
+print(f"OvR score matches manual calculation: {abs(scores_ovr - manual_score_ovr) < 1e-10}")
\ No newline at end of file
diff --git a/test_default.py b/test_default.py
new file mode 100644
index 000000000..88d621cfc
--- /dev/null
+++ b/test_default.py
@@ -0,0 +1,28 @@
+import numpy as np
+from sklearn import linear_model
+
+# Test that default behavior (OvR) still works
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b'], size=samples)  # Binary data
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+print("=== Testing default behavior (OvR) ===")
+
+# Test with default parameters (should use OvR)
+coefs, _, scores, _ = linear_model.logistic._log_reg_scoring_path(
+    x, y, train, test, fit_intercept=True, scoring='neg_log_loss'
+)
+
+c_index = 0
+score = scores[c_index]
+
+print(f"Score from _log_reg_scoring_path: {score}")
+print("Default test completed successfully (no errors)")
\ No newline at end of file
diff --git a/test_ovr_case.py b/test_ovr_case.py
new file mode 100644
index 000000000..c091d46b0
--- /dev/null
+++ b/test_ovr_case.py
@@ -0,0 +1,48 @@
+import numpy as np
+from sklearn import preprocessing, linear_model
+
+# Test that OvR case still works
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b'], size=samples)  # Binary case for OvR
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+# Test OvR case with binary data
+print("=== Testing OvR case with binary data ===")
+coefs_ovr, _, scores_ovr, _ = linear_model.logistic._log_reg_scoring_path(
+    x, y, train, test, fit_intercept=True, scoring='neg_log_loss', multi_class='ovr'
+)
+
+c_index = 0
+coefs_ovr = coefs_ovr[c_index]
+scores_ovr = scores_ovr[c_index]
+
+# Create a LogisticRegression with OvR to verify the score matches
+log_reg_ovr = linear_model.LogisticRegression(fit_intercept=True, multi_class='ovr')
+# For binary OvR, coefs_ovr is 1D array
+if coefs_ovr.ndim == 1:
+    log_reg_ovr.coef_ = coefs_ovr[:-1].reshape(1, -1)
+    log_reg_ovr.intercept_ = coefs_ovr[-1:]
+else:
+    log_reg_ovr.coef_ = coefs_ovr[:, :-1]
+    log_reg_ovr.intercept_ = coefs_ovr[:, -1]
+log_reg_ovr.classes_ = np.array([-1, 1])
+
+# Binarize the labels for y[test]
+lb = preprocessing.label.LabelBinarizer()
+lb.fit(y[test])
+y_bin = lb.transform(y[test])
+
+probs_ovr = log_reg_ovr.predict_proba(x[test])
+manual_score_ovr = (y_bin*np.log(probs_ovr)).sum(axis=1).mean()
+
+print(f"Score from _log_reg_scoring_path: {scores_ovr}")
+print(f"Manual calculation score: {manual_score_ovr}")
+print(f"Scores match: {abs(scores_ovr - manual_score_ovr) < 1e-10}")
\ No newline at end of file