diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..e4e3de5f8
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,64 @@
+#!/usr/bin/env python3
+"""
+Reproduction script for the SparseCoder max_iter issue.
+This script demonstrates that SparseCoder doesn't expose max_iter parameter
+for the Lasso algorithm (lasso_cd), causing convergence warnings.
+"""
+
+import numpy as np
+from sklearn.decomposition import SparseCoder
+from sklearn.linear_model import Lasso
+import warnings
+
+# Create a simple test case
+np.random.seed(42)
+n_samples, n_features, n_components = 10, 20, 15
+X = np.random.randn(n_samples, n_features)
+dictionary = np.random.randn(n_components, n_features)
+
+print("Testing SparseCoder with lasso_cd algorithm...")
+
+# Test with SparseCoder using lasso_cd
+coder = SparseCoder(
+    dictionary=dictionary,
+    transform_algorithm='lasso_cd',
+    transform_alpha=0.1
+)
+
+# This should trigger a convergence warning since max_iter defaults to 1000
+with warnings.catch_warnings(record=True) as w:
+    warnings.simplefilter("always")
+    result = coder.transform(X)
+    if w:
+        print(f"Warning captured: {w[-1].message}")
+        print(f"Warning category: {w[-1].category}")
+    else:
+        print("No warnings captured")
+
+print("Testing Lasso directly to show it accepts max_iter...")
+# Compare with direct Lasso usage
+lasso = Lasso(alpha=0.1, max_iter=5000)  # This accepts max_iter parameter
+lasso.fit(dictionary.T, X.T)
+print("Lasso fitted successfully with custom max_iter")
+
+print("Testing if SparseCoder accepts max_iter parameter (it should now)...")
+try:
+    # This should now work since we added max_iter parameter
+    coder_with_max_iter = SparseCoder(
+        dictionary=dictionary,
+        transform_algorithm='lasso_cd',
+        transform_alpha=0.1,
+        max_iter=5000  # This parameter should now be accepted
+    )
+    print("SUCCESS: SparseCoder now accepts max_iter parameter")
+    
+    # Test that the max_iter value is stored correctly
+    if hasattr(coder_with_max_iter, 'max_iter') and coder_with_max_iter.max_iter == 5000:
+        print("SUCCESS: max_iter value is stored correctly")
+    else:
+        print("ERROR: max_iter value not stored correctly")
+        
+except TypeError as e:
+    print(f"ERROR: SparseCoder still doesn't accept max_iter parameter: {e}")
+
+print("\nReproduction script completed.")
\ No newline at end of file
diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py
index a318c957f..18e3d2b2a 100644
--- a/sklearn/decomposition/dict_learning.py
+++ b/sklearn/decomposition/dict_learning.py
@@ -865,7 +865,7 @@ class SparseCodingMixin(TransformerMixin):
                                   transform_algorithm='omp',
                                   transform_n_nonzero_coefs=None,
                                   transform_alpha=None, split_sign=False,
-                                  n_jobs=None, positive_code=False):
+                                  n_jobs=None, positive_code=False, max_iter=1000):
         self.n_components = n_components
         self.transform_algorithm = transform_algorithm
         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs
@@ -873,6 +873,7 @@ class SparseCodingMixin(TransformerMixin):
         self.split_sign = split_sign
         self.n_jobs = n_jobs
         self.positive_code = positive_code
+        self.max_iter = max_iter
 
     def transform(self, X):
         """Encode the data as a sparse combination of the dictionary atoms.
@@ -900,7 +901,7 @@ class SparseCodingMixin(TransformerMixin):
             X, self.components_, algorithm=self.transform_algorithm,
             n_nonzero_coefs=self.transform_n_nonzero_coefs,
             alpha=self.transform_alpha, n_jobs=self.n_jobs,
-            positive=self.positive_code)
+            positive=self.positive_code, max_iter=self.max_iter)
 
         if self.split_sign:
             # feature vector is split into a positive and negative side
@@ -974,6 +975,9 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
         .. versionadded:: 0.20
 
+    max_iter : int, optional (default=1000)
+        Maximum number of iterations to perform if `algorithm='lasso_cd'`.
+
     Attributes
     ----------
     components_ : array, [n_components, n_features]
@@ -991,12 +995,12 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):
 
     def __init__(self, dictionary, transform_algorithm='omp',
                  transform_n_nonzero_coefs=None, transform_alpha=None,
-                 split_sign=False, n_jobs=None, positive_code=False):
+                 split_sign=False, n_jobs=None, positive_code=False, max_iter=1000):
         self._set_sparse_coding_params(dictionary.shape[0],
                                        transform_algorithm,
                                        transform_n_nonzero_coefs,
                                        transform_alpha, split_sign, n_jobs,
-                                       positive_code)
+                                       positive_code, max_iter)
         self.components_ = dictionary
 
     def fit(self, X, y=None):
diff --git a/test_convergence_warning.py b/test_convergence_warning.py
new file mode 100644
index 000000000..741c542d1
--- /dev/null
+++ b/test_convergence_warning.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python3
+"""
+Test script to demonstrate that the max_iter parameter can now be used
+to avoid convergence warnings in SparseCoder with lasso_cd algorithm.
+"""
+
+import numpy as np
+from sklearn.decomposition import SparseCoder
+import warnings
+
+# Create a more challenging test case that might trigger convergence issues
+np.random.seed(42)
+n_samples, n_features, n_components = 100, 50, 40
+X = np.random.randn(n_samples, n_features)
+dictionary = np.random.randn(n_components, n_features)
+
+print("Testing convergence behavior with different max_iter values...")
+
+# Test with default max_iter (1000)
+print("\n1. Testing with default max_iter=1000:")
+with warnings.catch_warnings(record=True) as w:
+    warnings.simplefilter("always")
+    coder_default = SparseCoder(
+        dictionary=dictionary,
+        transform_algorithm='lasso_cd',
+        transform_alpha=0.01  # Lower alpha for more challenging optimization
+    )
+    result_default = coder_default.transform(X)
+    
+    convergence_warnings = [warning for warning in w if 'convergence' in str(warning.message).lower()]
+    if convergence_warnings:
+        print(f"   Convergence warning: {convergence_warnings[0].message}")
+    else:
+        print("   No convergence warnings")
+
+# Test with increased max_iter to avoid convergence issues
+print("\n2. Testing with increased max_iter=5000:")
+with warnings.catch_warnings(record=True) as w:
+    warnings.simplefilter("always")
+    coder_high_iter = SparseCoder(
+        dictionary=dictionary,
+        transform_algorithm='lasso_cd',
+        transform_alpha=0.01,
+        max_iter=5000  # Increased max_iter to avoid convergence issues
+    )
+    result_high_iter = coder_high_iter.transform(X)
+    
+    convergence_warnings = [warning for warning in w if 'convergence' in str(warning.message).lower()]
+    if convergence_warnings:
+        print(f"   Convergence warning: {convergence_warnings[0].message}")
+    else:
+        print("   No convergence warnings")
+
+# Test with very low max_iter to force convergence warning
+print("\n3. Testing with very low max_iter=10 (should trigger warning):")
+with warnings.catch_warnings(record=True) as w:
+    warnings.simplefilter("always")
+    coder_low_iter = SparseCoder(
+        dictionary=dictionary,
+        transform_algorithm='lasso_cd',
+        transform_alpha=0.01,
+        max_iter=10  # Very low max_iter to force convergence warning
+    )
+    result_low_iter = coder_low_iter.transform(X)
+    
+    convergence_warnings = [warning for warning in w if 'convergence' in str(warning.message).lower()]
+    if convergence_warnings:
+        print(f"   Convergence warning: {convergence_warnings[0].message}")
+    else:
+        print("   No convergence warnings (unexpected)")
+
+print("\nTest completed. The max_iter parameter is now properly exposed and functional!")
\ No newline at end of file
diff --git a/test_max_iter_functionality.py b/test_max_iter_functionality.py
new file mode 100644
index 000000000..6ea799644
--- /dev/null
+++ b/test_max_iter_functionality.py
@@ -0,0 +1,71 @@
+#!/usr/bin/env python3
+"""
+Test script to verify that the max_iter parameter is properly passed through
+from SparseCoder to the underlying sparse_encode function.
+"""
+
+import numpy as np
+from sklearn.decomposition import SparseCoder
+from sklearn.decomposition.dict_learning import sparse_encode
+
+# Create test data
+np.random.seed(42)
+n_samples, n_features, n_components = 10, 20, 15
+X = np.random.randn(n_samples, n_features)
+dictionary = np.random.randn(n_components, n_features)
+
+print("Testing max_iter parameter functionality...")
+
+# Test 1: Default max_iter value (should be 1000)
+coder_default = SparseCoder(
+    dictionary=dictionary,
+    transform_algorithm='lasso_cd',
+    transform_alpha=0.1
+)
+print(f"Default max_iter: {coder_default.max_iter}")
+
+# Test 2: Custom max_iter value
+custom_max_iter = 2500
+coder_custom = SparseCoder(
+    dictionary=dictionary,
+    transform_algorithm='lasso_cd',
+    transform_alpha=0.1,
+    max_iter=custom_max_iter
+)
+print(f"Custom max_iter: {coder_custom.max_iter}")
+
+# Test 3: Verify that transform method uses the max_iter parameter
+# by comparing with direct sparse_encode call
+print("\nTesting that transform method passes max_iter correctly...")
+
+# Call transform method which should use the stored max_iter
+result_from_coder = coder_custom.transform(X)
+
+# Call sparse_encode directly with the same max_iter
+result_direct = sparse_encode(
+    X, dictionary, algorithm='lasso_cd',
+    alpha=0.1, max_iter=custom_max_iter
+)
+
+# The results should be similar (allowing for minor numerical differences)
+difference = np.abs(result_from_coder - result_direct).max()
+print(f"Max difference between coder.transform and direct sparse_encode: {difference}")
+
+if difference < 1e-10:
+    print("SUCCESS: max_iter parameter is being passed through correctly")
+else:
+    print("WARNING: Results differ significantly - check max_iter parameter passing")
+
+# Test 4: Test with different max_iter values
+print("\nTesting with different max_iter values...")
+for test_max_iter in [500, 1000, 2000]:
+    test_coder = SparseCoder(
+        dictionary=dictionary,
+        transform_algorithm='lasso_cd',
+        transform_alpha=0.1,
+        max_iter=test_max_iter
+    )
+    result = test_coder.transform(X)
+    print(f"max_iter={test_max_iter}: transform completed successfully")
+
+print("\nAll tests completed successfully!")
\ No newline at end of file