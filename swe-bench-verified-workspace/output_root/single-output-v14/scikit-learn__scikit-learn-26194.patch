diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..5a1ea2d86
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,20 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+def test_roc_curve_with_probability_estimates():
+    rng = np.random.RandomState(42)
+    y_true = rng.randint(0, 2, size=10)
+    y_score = rng.rand(10)
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print("y_true:", y_true)
+    print("y_score:", y_score)
+    print("thresholds:", thresholds)
+    print("Thresholds > 1:", thresholds[thresholds > 1])
+    print("Thresholds <= 1:", thresholds[thresholds <= 1])
+    
+    # Test the assertion from the issue
+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all(), \
+        f"Some thresholds are not in [0, 1]: {thresholds}"
+
+if __name__ == "__main__":
+    test_roc_curve_with_probability_estimates()
\ No newline at end of file
diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py
index 0cdead923..bea271bc3 100644
--- a/sklearn/metrics/_ranking.py
+++ b/sklearn/metrics/_ranking.py
@@ -1083,7 +1083,13 @@ def roc_curve(
     # to make sure that the curve starts at (0, 0)
     tps = np.r_[0, tps]
     fps = np.r_[0, fps]
-    thresholds = np.r_[thresholds[0] + 1, thresholds]
+    
+    # Ensure thresholds do not exceed 1 when dealing with probability estimates
+    # Only clip if the maximum threshold is <= 1 (indicating probability estimates)
+    if np.max(thresholds) <= 1:
+        thresholds = np.r_[min(thresholds[0] + 1, 1), thresholds]
+    else:
+        thresholds = np.r_[thresholds[0] + 1, thresholds]
 
     if fps[-1] <= 0:
         warnings.warn(
diff --git a/test_edge_cases.py b/test_edge_cases.py
new file mode 100644
index 000000000..edff3b35c
--- /dev/null
+++ b/test_edge_cases.py
@@ -0,0 +1,42 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+def test_edge_cases():
+    print("Testing edge cases for roc_curve thresholds...")
+    
+    # Test case 1: All scores are 1.0
+    print("\n1. All scores = 1.0")
+    y_true = np.array([0, 1, 0, 1])
+    y_score = np.array([1.0, 1.0, 1.0, 1.0])
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print("Thresholds:", thresholds)
+    assert np.all(thresholds <= 1), f"Thresholds exceed 1: {thresholds}"
+    
+    # Test case 2: All scores are 0.0
+    print("\n2. All scores = 0.0")
+    y_true = np.array([0, 1, 0, 1])
+    y_score = np.array([0.0, 0.0, 0.0, 0.0])
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print("Thresholds:", thresholds)
+    assert np.all(thresholds <= 1), f"Thresholds exceed 1: {thresholds}"
+    
+    # Test case 3: Mixed scores including 1.0
+    print("\n3. Mixed scores including 1.0")
+    y_true = np.array([0, 1, 0, 1, 0])
+    y_score = np.array([0.5, 0.8, 1.0, 0.3, 0.9])
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print("Thresholds:", thresholds)
+    assert np.all(thresholds <= 1), f"Thresholds exceed 1: {thresholds}"
+    
+    # Test case 4: Very small scores
+    print("\n4. Very small scores")
+    y_true = np.array([0, 1, 0, 1])
+    y_score = np.array([0.001, 0.0001, 0.0005, 0.002])
+    _, _, thresholds = roc_curve(y_true, y_score)
+    print("Thresholds:", thresholds)
+    assert np.all(thresholds <= 1), f"Thresholds exceed 1: {thresholds}"
+    
+    print("\nAll edge case tests passed!")
+
+if __name__ == "__main__":
+    test_edge_cases()
\ No newline at end of file
diff --git a/test_failing_case.py b/test_failing_case.py
new file mode 100644
index 000000000..c2473b2e9
--- /dev/null
+++ b/test_failing_case.py
@@ -0,0 +1,17 @@
+import numpy as np
+from sklearn.metrics import roc_curve
+
+def test_failing_case():
+    # Test case from the failing test
+    y_true = [0, 0, 0, 0, 1, 1]
+    y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]
+    tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)
+    print("Thresholds:", thresholds)
+    print("Expected: [2.0, 1.0, 0.7, 0.0]")
+    
+    # Check if this is a probability estimate case (max threshold <= 1)
+    print("Max threshold:", np.max(thresholds))
+    print("Is probability estimate (max <= 1):", np.max(thresholds) <= 1)
+
+if __name__ == "__main__":
+    test_failing_case()
\ No newline at end of file