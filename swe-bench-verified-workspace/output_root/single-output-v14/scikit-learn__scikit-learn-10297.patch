diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..8053005ec
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,16 @@
+import numpy as np
+from sklearn import linear_model as lm
+
+# test database
+n = 100
+x = np.random.randn(n, 30)
+y = np.random.randint(0, 2, size=n)  # Binary classification labels
+
+# This should reproduce the issue
+try:
+    rr = lm.RidgeClassifierCV(alphas=np.arange(0.1, 1000, 0.1), normalize=True, 
+                             store_cv_values=True).fit(x, y)
+    print("No error occurred - store_cv_values parameter is accepted")
+except TypeError as e:
+    print(f"Error occurred: {e}")
+    print("This confirms the issue exists")
\ No newline at end of file
diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 911c74ae3..6469cdbcf 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -1301,6 +1301,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         weights inversely proportional to class frequencies in the input data
         as ``n_samples / (n_classes * np.bincount(y))``
 
+    store_cv_values : boolean, default=False
+        Flag indicating if the cross-validation values corresponding to
+        each alpha should be stored in the `cv_values_` attribute (see
+        below). This flag is only compatible with `cv=None` (i.e. using
+        Generalized Cross-Validation).
+
     Attributes
     ----------
     cv_values_ : array, shape = [n_samples, n_alphas] or \
@@ -1333,10 +1339,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     advantage of the multi-variate response support in Ridge.
     """
     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,
-                 normalize=False, scoring=None, cv=None, class_weight=None):
+                 normalize=False, scoring=None, cv=None, class_weight=None,
+                 store_cv_values=False):
         super(RidgeClassifierCV, self).__init__(
             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,
-            scoring=scoring, cv=cv)
+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)
         self.class_weight = class_weight
 
     def fit(self, X, y, sample_weight=None):
diff --git a/test_edge_cases.py b/test_edge_cases.py
new file mode 100644
index 000000000..3aed5807a
--- /dev/null
+++ b/test_edge_cases.py
@@ -0,0 +1,46 @@
+import numpy as np
+from sklearn import linear_model as lm
+
+# Test database
+n = 100
+x = np.random.randn(n, 30)
+y = np.random.randint(0, 3, size=n)  # Multi-class classification labels
+
+print("Testing edge cases...")
+
+# Test 1: Multi-class classification with store_cv_values=True
+print("\n1. Multi-class classification with store_cv_values=True:")
+try:
+    rr = lm.RidgeClassifierCV(alphas=np.arange(0.1, 5, 0.1), 
+                             store_cv_values=True).fit(x, y)
+    print("✓ Multi-class with store_cv_values=True works")
+    print(f"  cv_values_ shape: {rr.cv_values_.shape}")
+    print(f"  Number of classes: {len(rr.classes_)}")
+except Exception as e:
+    print(f"✗ Error: {e}")
+
+# Test 2: Using cv parameter (not None) with store_cv_values=True - should raise error
+print("\n2. Using cv parameter with store_cv_values=True (should raise error):")
+try:
+    rr = lm.RidgeClassifierCV(alphas=np.arange(0.1, 5, 0.1), cv=3,
+                             store_cv_values=True).fit(x, y)
+    print("✗ Should have raised an error but didn't")
+except ValueError as e:
+    print(f"✓ Correctly raised ValueError: {e}")
+except Exception as e:
+    print(f"✗ Unexpected error: {e}")
+
+# Test 3: Using cv parameter with store_cv_values=False - should work
+print("\n3. Using cv parameter with store_cv_values=False:")
+try:
+    rr = lm.RidgeClassifierCV(alphas=np.arange(0.1, 5, 0.1), cv=3,
+                             store_cv_values=False).fit(x, y)
+    print("✓ cv parameter with store_cv_values=False works")
+    if not hasattr(rr, 'cv_values_'):
+        print("✓ cv_values_ attribute correctly does not exist")
+    else:
+        print("✗ cv_values_ attribute exists when it shouldn't")
+except Exception as e:
+    print(f"✗ Error: {e}")
+
+print("\nAll edge case tests completed!")
\ No newline at end of file
diff --git a/test_store_cv_values.py b/test_store_cv_values.py
new file mode 100644
index 000000000..16f9a3f97
--- /dev/null
+++ b/test_store_cv_values.py
@@ -0,0 +1,59 @@
+import numpy as np
+from sklearn import linear_model as lm
+
+# Test database
+n = 100
+x = np.random.randn(n, 30)
+y = np.random.randint(0, 2, size=n)  # Binary classification labels
+
+print("Testing RidgeClassifierCV with store_cv_values=True...")
+
+# Test with store_cv_values=True
+try:
+    rr = lm.RidgeClassifierCV(alphas=np.arange(0.1, 10, 0.1), normalize=True, 
+                             store_cv_values=True).fit(x, y)
+    print("✓ store_cv_values=True parameter accepted")
+    
+    # Check if cv_values_ attribute exists
+    if hasattr(rr, 'cv_values_'):
+        print(f"✓ cv_values_ attribute exists with shape: {rr.cv_values_.shape}")
+        print(f"✓ Number of alphas: {len(rr.alphas)}")
+        print(f"✓ cv_values_ shape matches expectations: {rr.cv_values_.shape}")
+    else:
+        print("✗ cv_values_ attribute does not exist")
+        
+except Exception as e:
+    print(f"✗ Error with store_cv_values=True: {e}")
+
+print("\nTesting RidgeClassifierCV with store_cv_values=False...")
+
+# Test with store_cv_values=False (default)
+try:
+    rr2 = lm.RidgeClassifierCV(alphas=np.arange(0.1, 10, 0.1), normalize=True, 
+                              store_cv_values=False).fit(x, y)
+    print("✓ store_cv_values=False parameter accepted")
+    
+    # Check if cv_values_ attribute exists
+    if hasattr(rr2, 'cv_values_'):
+        print(f"✗ cv_values_ attribute exists when it shouldn't: {rr2.cv_values_.shape}")
+    else:
+        print("✓ cv_values_ attribute correctly does not exist")
+        
+except Exception as e:
+    print(f"✗ Error with store_cv_values=False: {e}")
+
+print("\nTesting RidgeClassifierCV without store_cv_values (default behavior)...")
+
+# Test without specifying store_cv_values (should default to False)
+try:
+    rr3 = lm.RidgeClassifierCV(alphas=np.arange(0.1, 10, 0.1), normalize=True).fit(x, y)
+    print("✓ Default parameter behavior works")
+    
+    # Check if cv_values_ attribute exists
+    if hasattr(rr3, 'cv_values_'):
+        print(f"✗ cv_values_ attribute exists when it shouldn't: {rr3.cv_values_.shape}")
+    else:
+        print("✓ cv_values_ attribute correctly does not exist")
+        
+except Exception as e:
+    print(f"✗ Error with default behavior: {e}")
\ No newline at end of file