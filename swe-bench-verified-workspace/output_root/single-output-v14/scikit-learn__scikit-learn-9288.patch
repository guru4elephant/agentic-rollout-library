diff --git a/comprehensive_test.py b/comprehensive_test.py
new file mode 100644
index 000000000..5c07ff41c
--- /dev/null
+++ b/comprehensive_test.py
@@ -0,0 +1,28 @@
+from sklearn.cluster import KMeans
+from sklearn.datasets import make_blobs
+import numpy as np
+
+def test_scenario(description, **kwargs):
+    print(f"\n=== {description} ===")
+    X, y = make_blobs(n_samples=1000, centers=8, n_features=4, random_state=42)
+    
+    results = {}
+    for n_jobs in [1, 2, 3, 4]:
+        kmeans = KMeans(**kwargs, n_jobs=n_jobs)
+        kmeans.fit(X)
+        results[n_jobs] = kmeans.inertia_
+        print(f"n_jobs={n_jobs}: {kmeans.inertia_}")
+    
+    # Check if all results are the same
+    all_same = all(np.isclose(results[1], results[n_jobs]) for n_jobs in results)
+    print(f"All n_jobs values give same result: {all_same}")
+    return all_same
+
+# Test different scenarios
+test1 = test_scenario("Default parameters", n_clusters=8, random_state=42)
+test2 = test_scenario("Different algorithm", n_clusters=8, random_state=42, algorithm="elkan")
+test3 = test_scenario("Multiple initializations", n_clusters=8, random_state=42, n_init=5)
+test4 = test_scenario("Different init method", n_clusters=8, random_state=42, init="random")
+
+print(f"\n=== SUMMARY ===")
+print(f"All tests passed: {all([test1, test2, test3, test4])}")
\ No newline at end of file
diff --git a/debug_random.py b/debug_random.py
new file mode 100644
index 000000000..201419e47
--- /dev/null
+++ b/debug_random.py
@@ -0,0 +1,19 @@
+from sklearn.cluster import KMeans
+from sklearn.datasets import make_blobs
+import numpy as np
+
+# Test with more distinct random states
+X, y = make_blobs(n_samples=1000, centers=5, n_features=2, random_state=42)
+
+print("Testing with very different random states:")
+for random_state in [1, 42, 999, 12345]:
+    kmeans = KMeans(n_clusters=5, random_state=random_state, n_jobs=1).fit(X)
+    print(f"Random state {random_state}: {kmeans.inertia_}")
+
+print("\nTesting initialization centers:")
+for random_state in [1, 42]:
+    kmeans = KMeans(n_clusters=5, random_state=random_state, n_jobs=1, init='random')
+    kmeans.fit(X)
+    print(f"Random state {random_state} centers (first few):")
+    print(kmeans.cluster_centers_[:2])
+    print(f"Inertia: {kmeans.inertia_}")
\ No newline at end of file
diff --git a/reproduce_issue.py b/reproduce_issue.py
new file mode 100644
index 000000000..31c7dacfd
--- /dev/null
+++ b/reproduce_issue.py
@@ -0,0 +1,11 @@
+from sklearn.cluster import KMeans
+from sklearn.datasets import make_blobs
+
+# Generate some data
+X, y = make_blobs(n_samples=10000, centers=10, n_features=2, random_state=2)
+
+# Run KMeans with various n_jobs values
+for n_jobs in range(1, 5):
+    kmeans = KMeans(n_clusters=10, random_state=2, n_jobs=n_jobs)
+    kmeans.fit(X)
+    print(f'(n_jobs={n_jobs}) kmeans.inertia_ = {kmeans.inertia_}')
\ No newline at end of file
diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py
index b7fbdf7da..71d15d142 100644
--- a/sklearn/cluster/k_means_.py
+++ b/sklearn/cluster/k_means_.py
@@ -360,6 +360,10 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
     else:
         raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got"
                          " %s" % str(algorithm))
+    # Generate seeds for all runs to ensure consistent random state handling
+    # regardless of n_jobs value
+    seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
+    
     if effective_n_jobs(n_jobs) == 1:
         # For a single thread, less memory is needed if we just store one set
         # of the best results (as opposed to one set per run per thread).
@@ -369,7 +373,7 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,
                 verbose=verbose, precompute_distances=precompute_distances,
                 tol=tol, x_squared_norms=x_squared_norms,
-                random_state=random_state)
+                random_state=seeds[it])
             # determine if these results are the best so far
             if best_inertia is None or inertia < best_inertia:
                 best_labels = labels.copy()
@@ -378,14 +382,13 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
                 best_n_iter = n_iter_
     else:
         # parallelisation of k-means runs
-        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
         results = Parallel(n_jobs=n_jobs, verbose=0)(
             delayed(kmeans_single)(X, sample_weight, n_clusters,
                                    max_iter=max_iter, init=init,
                                    verbose=verbose, tol=tol,
                                    precompute_distances=precompute_distances,
                                    x_squared_norms=x_squared_norms,
-                                   # Change seed to ensure variety
+                                   # Use pre-generated seeds for consistent results
                                    random_state=seed)
             for seed in seeds)
         # Get results with the lowest inertia
diff --git a/test_challenging.py b/test_challenging.py
new file mode 100644
index 000000000..da79a7b5f
--- /dev/null
+++ b/test_challenging.py
@@ -0,0 +1,22 @@
+from sklearn.cluster import KMeans
+from sklearn.datasets import make_blobs
+import numpy as np
+
+# Create a more challenging dataset with overlapping clusters
+X, y = make_blobs(n_samples=1000, centers=10, n_features=10, cluster_std=5.0, random_state=42)
+
+print("Testing with challenging dataset (overlapping clusters):")
+for random_state in [1, 2, 3, 4]:
+    kmeans = KMeans(n_clusters=10, random_state=random_state, n_jobs=1).fit(X)
+    print(f"Random state {random_state}: {kmeans.inertia_}")
+
+print("\nTesting n_jobs consistency:")
+results = {}
+for n_jobs in [1, 2, 3, 4]:
+    kmeans = KMeans(n_clusters=10, random_state=42, n_jobs=n_jobs).fit(X)
+    results[n_jobs] = kmeans.inertia_
+    print(f"n_jobs={n_jobs}: {kmeans.inertia_}")
+
+# Check if all results are the same
+all_same = all(np.isclose(results[1], results[n_jobs]) for n_jobs in results)
+print(f"All n_jobs values give same result: {all_same}")
\ No newline at end of file
diff --git a/test_fix.py b/test_fix.py
new file mode 100644
index 000000000..cbedc7925
--- /dev/null
+++ b/test_fix.py
@@ -0,0 +1,38 @@
+from sklearn.cluster import KMeans
+from sklearn.datasets import make_blobs
+import numpy as np
+
+# Test 1: Different random states should give different results
+print("Test 1: Different random states")
+X, y = make_blobs(n_samples=1000, centers=5, n_features=2, random_state=42)
+kmeans1 = KMeans(n_clusters=5, random_state=1, n_jobs=1).fit(X)
+kmeans2 = KMeans(n_clusters=5, random_state=2, n_jobs=1).fit(X)
+print(f"Random state 1: {kmeans1.inertia_}")
+print(f"Random state 2: {kmeans2.inertia_}")
+print(f"Different random states give different results: {not np.isclose(kmeans1.inertia_, kmeans2.inertia_)}")
+print()
+
+# Test 2: Same random state should give same results regardless of n_jobs
+print("Test 2: Same random state with different n_jobs")
+results = {}
+for n_jobs in [1, 2, 3, 4]:
+    kmeans = KMeans(n_clusters=5, random_state=42, n_jobs=n_jobs).fit(X)
+    results[n_jobs] = kmeans.inertia_
+    print(f"n_jobs={n_jobs}: {kmeans.inertia_}")
+
+# Check if all results are the same
+all_same = all(np.isclose(results[1], results[n_jobs]) for n_jobs in results)
+print(f"All n_jobs values give same result: {all_same}")
+print()
+
+# Test 3: Multiple runs with same parameters
+print("Test 3: Multiple runs with same parameters")
+inertias = []
+for i in range(5):
+    kmeans = KMeans(n_clusters=5, random_state=42, n_jobs=2).fit(X)
+    inertias.append(kmeans.inertia_)
+    print(f"Run {i+1}: {kmeans.inertia_}")
+
+# Check if all runs are identical (they should be with fixed random state)
+all_identical = all(np.isclose(inertias[0], inertia) for inertia in inertias)
+print(f"All runs with same parameters are identical: {all_identical}")
\ No newline at end of file