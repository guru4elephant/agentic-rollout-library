#!/usr/bin/env python3
"""
æµ‹è¯•GeneralAgentçš„debugæ¨¡å¼
éªŒè¯LLMè¾“å…¥è¾“å‡ºçš„è¯¦ç»†æ—¥å¿—è®°å½•
"""

import asyncio
import logging
import sys
import os

# Add the parent directory to the path so we can import workers module
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from workers.core import create_tool, create_agent
from workers.utils import create_llm_client

# è®¾ç½®DEBUGçº§åˆ«æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# APIé…ç½®
API_KEY = os.getenv("LLM_API_KEY", "your-api-key-here")
BASE_URL = os.getenv("LLM_BASE_URL", "your-base-url-here")
MODEL_NAME = "gpt-4.1"

async def test_debug_mode():
    """æµ‹è¯•GeneralAgentçš„debugæ¨¡å¼"""
    print("ğŸ” æµ‹è¯•GeneralAgent Debugæ¨¡å¼")
    print("=" * 60)
    
    # 1. åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = create_llm_client(
        api_key=API_KEY,
        base_url=BASE_URL,
        model=MODEL_NAME,
        debug=False,  # LLMå®¢æˆ·ç«¯ä¸å¼€debugï¼Œåªçœ‹Agentçš„debug
        max_retries=1
    )
    
    try:
        # 2. åˆ›å»ºç®€å•å·¥å…·
        tools = {
            "bash_executor": create_tool("BashExecutor", {
                "execution_mode": "local",
                "timeout": 10
            }),
            "finish": create_tool("Finish")
        }
        
        # 3. åˆ›å»ºAgentï¼ˆå¼€å¯debugæ¨¡å¼ï¼‰
        print("ğŸ¤– åˆ›å»ºGeneralAgent (debug=True)...")
        agent = create_agent("General", {
            "max_rounds": 2,  # åªæ‰§è¡Œ2è½®è¿›è¡Œæµ‹è¯•
            "debug": True,    # å¼€å¯debugæ¨¡å¼
            "termination_tool_names": ["finish"]
        })
        
        # 4. é…ç½®å·¥å…·
        agent.set_tools(tools)
        print(f"âœ… Agentå·²é…ç½®ï¼Œdebugæ¨¡å¼: {agent.debug}")
        
        # 5. æ‰§è¡Œç®€å•ä»»åŠ¡
        print("\nğŸš€ æ‰§è¡Œç®€å•ä»»åŠ¡...")
        print("-" * 40)
        
        trajectory = await agent.run_trajectory(
            prompt="è¯·æ‰§è¡Œlså‘½ä»¤æŸ¥çœ‹å½“å‰ç›®å½•ï¼Œç„¶åç”¨finishå·¥å…·ç»“æŸã€‚",
            llm_generate_func=llm_client.generate,
            request_id="debug_test_001"
        )
        
        print("-" * 40)
        print(f"âœ… ä»»åŠ¡å®Œæˆï¼Œæ€»æ­¥æ•°: {len(trajectory.steps)}")
        
        return trajectory
        
    finally:
        await llm_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª GeneralAgent Debugæ¨¡å¼æµ‹è¯•")
    print("=" * 80)
    print("""
ğŸ“‹ æµ‹è¯•ç›®æ ‡:
- éªŒè¯GeneralAgentçš„debugå‚æ•°å·¥ä½œæ­£å¸¸
- ç¡®è®¤æ¯æ¬¡LLMè°ƒç”¨çš„è¾“å…¥è¾“å‡ºéƒ½æœ‰è¯¦ç»†æ—¥å¿—
- æ£€æŸ¥debugæ—¥å¿—çš„æ ¼å¼å’Œå†…å®¹å®Œæ•´æ€§

ğŸ” é¢„æœŸçœ‹åˆ°çš„debugä¿¡æ¯:
- LLMè¾“å…¥: æ¶ˆæ¯å†…å®¹ã€å‚æ•°è®¾ç½®
- LLMè¾“å‡º: å“åº”å†…å®¹ã€é•¿åº¦ç»Ÿè®¡ã€å†…å®¹åˆ†æ
- è½®æ¬¡æ ‡è¯†å’Œæ—¶é—´æˆ³

å¼€å§‹æµ‹è¯•...
    """)
    
    try:
        trajectory = await test_debug_mode()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ Debugæ¨¡å¼æµ‹è¯•å®Œæˆï¼")
        print(f"âœ… æˆåŠŸæ‰§è¡Œäº† {len(trajectory.steps)} ä¸ªæ­¥éª¤")
        print("""
ğŸ’¡ DebugåŠŸèƒ½éªŒè¯:
- âœ… GeneralAgent debugå‚æ•°ä¼ é€’æ­£å¸¸
- âœ… LLMè¾“å…¥è¾“å‡ºè¯¦ç»†æ—¥å¿—è®°å½•  
- âœ… è½®æ¬¡æ ‡è¯†å’Œå†…å®¹åˆ†æ
- âœ… è°ƒè¯•ä¿¡æ¯æ ¼å¼åŒ–è‰¯å¥½

æŸ¥çœ‹ä¸Šé¢çš„è¯¦ç»†debugæ—¥å¿—è¾“å‡ºï¼
        """)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ” GeneralAgent Debugæ¨¡å¼æµ‹è¯•")
    print("éªŒè¯LLMè°ƒç”¨çš„è¯¦ç»†æ—¥å¿—è®°å½•åŠŸèƒ½")
    
    asyncio.run(main())