#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›åçš„GeneralAgentç³»ç»Ÿæç¤ºè¯
éªŒè¯å·¥å…·æè¿°å’ŒReActæ ¼å¼è§„èŒƒ
"""

import asyncio
import logging
import sys
import os

# Add the parent directory to the path so we can import workers module
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from workers.core import create_tool, create_agent
from workers.agents.general_agent import dump_trajectory
from workers.utils import create_llm_client

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# APIé…ç½®
API_KEY = os.getenv("LLM_API_KEY", "your-api-key-here")
BASE_URL = os.getenv("LLM_BASE_URL", "your-base-url-here")
#MODEL_NAME = "claude-sonnet-4-20250514"
MODEL_NAME = "gpt-4.1"


async def test_system_prompt_generation():
    """æµ‹è¯•ç³»ç»Ÿæç¤ºè¯ç”ŸæˆåŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•ç³»ç»Ÿæç¤ºè¯ç”Ÿæˆ")
    print("=" * 60)
    
    # 1. åˆ›å»ºå·¥å…·
    tools = {
        "bash_executor": create_tool("BashExecutor", {
            "execution_mode": "local",
            "timeout": 30
        }),
        "finish": create_tool("Finish")
    }
    
    # 2. åˆ›å»ºAgent
    agent = create_agent("General", {
        "max_rounds": 3,
        "termination_tool_names": ["finish"]
    })
    
    # 3. è®¾ç½®å·¥å…·
    agent.set_tools(tools)
    
    # 4. ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
    system_prompt = agent.create_system_prompt()
    
    print("ğŸ“‹ ç”Ÿæˆçš„ç³»ç»Ÿæç¤ºè¯:")
    print("-" * 60)
    print(system_prompt)
    print("-" * 60)
    
    # 5. éªŒè¯æç¤ºè¯å†…å®¹
    checks = [
        ("ReActæ¡†æ¶è¯´æ˜", "ReAct" in system_prompt),
        ("Thoughtæ ¼å¼è¦æ±‚", "Thought:" in system_prompt),
        ("Actionæ ¼å¼è¦æ±‚", "Action:" in system_prompt),
        ("å·¥å…·åˆ—è¡¨", "Available Tools" in system_prompt),
        ("bash_executorå·¥å…·", "bash_executor" in system_prompt),
        ("finishå·¥å…·", "finish" in system_prompt),
        ("å‚æ•°æ ¼å¼è¯´æ˜", "param=value" in system_prompt),
        ("ç¤ºä¾‹æµç¨‹", "Example ReAct Flow" in system_prompt),
    ]
    
    print("\nâœ… ç³»ç»Ÿæç¤ºè¯å†…å®¹éªŒè¯:")
    for check_name, result in checks:
        status = "âœ…" if result else "âŒ"
        print(f"   {status} {check_name}: {'é€šè¿‡' if result else 'å¤±è´¥'}")
    
    # 6. ç»Ÿè®¡ä¿¡æ¯
    prompt_length = len(system_prompt)
    prompt_lines = len(system_prompt.split('\n'))
    
    print(f"\nğŸ“Š æç¤ºè¯ç»Ÿè®¡:")
    print(f"   æ€»é•¿åº¦: {prompt_length} å­—ç¬¦")
    print(f"   æ€»è¡Œæ•°: {prompt_lines} è¡Œ")
    
    return system_prompt


async def test_agent_with_improved_prompt():
    """æµ‹è¯•ä½¿ç”¨æ”¹è¿›æç¤ºè¯çš„Agentæ‰§è¡Œæ•ˆæœ"""
    print("\nğŸš€ æµ‹è¯•æ”¹è¿›åçš„Agentæ‰§è¡Œ")
    print("=" * 60)
    
    # 1. åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        # 2. åˆ›å»ºå·¥å…·
        tools = {
            "bash_executor": create_tool("BashExecutor", {
                "execution_mode": "local",
                "timeout": 15
            }),
            "finish": create_tool("Finish")
        }
        
        # 3. åˆ›å»ºAgentï¼ˆä¸ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯ï¼Œè®©å®ƒä½¿ç”¨æ”¹è¿›åçš„é»˜è®¤æç¤ºè¯ï¼‰
        agent = create_agent("General", {
            "max_rounds": 5,
            "termination_tool_names": ["finish"]
        })
        
        # 4. è®¾ç½®å·¥å…·
        agent.set_tools(tools)
        print(f"âœ… é…ç½®äº† {len(agent.tools)} ä¸ªå·¥å…·")
        
        # 5. æ‰§è¡Œä»»åŠ¡
        print("\nğŸ“¤ å¼€å§‹æ‰§è¡Œä»»åŠ¡...")
        trajectory = await agent.run_trajectory(
            prompt="è¯·ä½¿ç”¨ReActæ ¼å¼è·å–å½“å‰ç³»ç»Ÿçš„ç”¨æˆ·åå’ŒPythonç‰ˆæœ¬ï¼Œç„¶åæä¾›æ€»ç»“æŠ¥å‘Šã€‚",
            llm_generate_func=llm_client.generate,
            request_id="improved_prompt_test_001"
        )
        
        # 6. åˆ†æç»“æœ
        print(f"\nâœ… ä»»åŠ¡å®Œæˆ!")
        print(f"   è½¨è¿¹çŠ¶æ€: {'å®Œæˆ' if trajectory.is_completed else 'æœªå®Œæˆ'}")
        print(f"   æ€»æ­¥æ•°: {len(trajectory.steps)}")
        print(f"   æ€»tokens: {trajectory.total_tokens}")
        
        # 7. åˆ†æReActæ ¼å¼ä½¿ç”¨æƒ…å†µ
        thought_steps = [s for s in trajectory.steps if s.step_type.value == "thought"]
        action_steps = [s for s in trajectory.steps if s.step_type.value == "action"]
        
        print(f"\nğŸ“‹ ReActæ ¼å¼åˆ†æ:")
        print(f"   Thoughtæ­¥æ•°: {len(thought_steps)}")
        print(f"   Actionæ­¥æ•°: {len(action_steps)}")
        
        # æ£€æŸ¥æ ¼å¼è§„èŒƒæ€§
        proper_format_count = 0
        for step in trajectory.steps:
            if step.content and (step.content.startswith("Thought:") or step.content.startswith("Action:")):
                proper_format_count += 1
        
        print(f"   æ­£ç¡®æ ¼å¼æ­¥æ•°: {proper_format_count}")
        print(f"   æ ¼å¼è§„èŒƒç‡: {proper_format_count/len(trajectory.steps)*100:.1f}%")
        
        # 8. æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ
        final_answer = None
        for step in reversed(trajectory.steps):
            if step.tool_name == "finish" and step.tool_result:
                final_answer = step.tool_result.get("answer")
                break
        
        if final_answer:
            print(f"\n=== æœ€ç»ˆæŠ¥å‘Š ===")
            print(final_answer)
        
        # 9. ä¿å­˜è½¨è¿¹
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"tests/improved_prompt_test_{timestamp}.json"
        txt_filename = f"tests/improved_prompt_test_{timestamp}.txt"
        
        dump_trajectory(trajectory, json_filename, "json")
        dump_trajectory(trajectory, txt_filename, "txt")
        print(f"\nğŸ’¾ è½¨è¿¹å·²ä¿å­˜åˆ° {json_filename} å’Œ {txt_filename}")
        
        return trajectory
        
    finally:
        await llm_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ”¹è¿›åçš„GeneralAgentç³»ç»Ÿæç¤ºè¯æµ‹è¯•")
    print("=" * 80)
    print("""
ğŸ“‹ æµ‹è¯•å†…å®¹:
1. âœ… éªŒè¯ç³»ç»Ÿæç¤ºè¯åŒ…å«å®Œæ•´çš„å·¥å…·æè¿°
2. âœ… éªŒè¯ReActæ ¼å¼è§„èŒƒå’Œè¦æ±‚
3. âœ… éªŒè¯å·¥å…·è°ƒç”¨æ ¼å¼è¯´æ˜
4. âœ… æµ‹è¯•Agentä½¿ç”¨æ”¹è¿›æç¤ºè¯çš„æ‰§è¡Œæ•ˆæœ
5. âœ… åˆ†æReActæ ¼å¼çš„ä½¿ç”¨æƒ…å†µ

æ”¹è¿›è¦ç‚¹:
- è¯¦ç»†çš„å·¥å…·å‚æ•°è¯´æ˜
- æ˜ç¡®çš„ReActæ ¼å¼è¦æ±‚
- å…·ä½“çš„å·¥å…·è°ƒç”¨ç¤ºä¾‹
- å®Œæ•´çš„æµç¨‹è¯´æ˜
    """)
    
    try:
        # æµ‹è¯•ç³»ç»Ÿæç¤ºè¯ç”Ÿæˆ
        system_prompt = await test_system_prompt_generation()
        
        # æµ‹è¯•Agentæ‰§è¡Œæ•ˆæœ
        trajectory = await test_agent_with_improved_prompt()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ”¹è¿›åçš„ç³»ç»Ÿæç¤ºè¯æµ‹è¯•å®Œæˆï¼")
        print(f"âœ… ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
        print(f"âœ… Agentæ‰§è¡Œæ­¥æ•°: {len(trajectory.steps)}")
        print("""
ğŸ’¡ ä¸»è¦æ”¹è¿›:
- ğŸ“š è¯¦ç»†çš„å·¥å…·åŠŸèƒ½å’Œå‚æ•°è¯´æ˜
- ğŸ“ æ˜ç¡®çš„ReActæ ¼å¼è¦æ±‚ (Thought: / Action:)
- ğŸ”§ å…·ä½“çš„å·¥å…·è°ƒç”¨æ ¼å¼ (tool_name(param=value))
- ğŸ“– å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œæµç¨‹è¯´æ˜
- ğŸ¯ æ›´å¥½çš„æ ¼å¼è§„èŒƒå’Œé”™è¯¯å¤„ç†æŒ‡å¯¼

è¿™äº›æ”¹è¿›å°†å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨ReActæ¡†æ¶ï¼
        """)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ”§ æ”¹è¿›åçš„GeneralAgentç³»ç»Ÿæç¤ºè¯æµ‹è¯•")
    print("éªŒè¯å·¥å…·æè¿°ã€ReActæ ¼å¼è§„èŒƒå’Œæ‰§è¡Œæ•ˆæœ")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import openai
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…openaiåº“: pip install openai")
        sys.exit(1) 
    
    asyncio.run(main())
