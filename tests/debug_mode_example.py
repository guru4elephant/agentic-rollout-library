#!/usr/bin/env python3
"""
LLMAPIClient Debugæ¨¡å¼ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨debugæ¨¡å¼æŸ¥çœ‹APIè°ƒç”¨çš„è¯¦ç»†è¾“å…¥å’Œè¾“å‡º
"""

import asyncio
import logging
import sys
import os

# Add the parent directory to the path so we can import workers module
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from workers.utils import create_llm_client

# APIé…ç½®
API_KEY = os.getenv("LLM_API_KEY", "your-api-key-here")
BASE_URL = os.getenv("LLM_BASE_URL", "your-base-url-here")
MODEL_NAME = os.getenv("LLM_MODEL_NAME", "claude-3-sonnet")


def setup_logging(debug: bool = False):
    """è®¾ç½®æ—¥å¿—çº§åˆ«"""
    # é‡æ–°é…ç½®æ—¥å¿—ï¼Œå¼ºåˆ¶åˆ·æ–°
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    if debug:
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S',
            force=True
        )
        # ç¡®ä¿workers.utils.llm_clientçš„æ—¥å¿—çº§åˆ«ä¹Ÿæ˜¯DEBUG
        logging.getLogger('workers.utils.llm_client').setLevel(logging.DEBUG)
        print("ğŸ› Debugæ¨¡å¼å·²å¯ç”¨ - å°†æ˜¾ç¤ºè¯¦ç»†çš„APIè°ƒç”¨æ—¥å¿—")
    else:
        logging.basicConfig(
            level=logging.INFO,
            format='%(levelname)s - %(message)s',
            force=True
        )
        print("â„¹ï¸  æ ‡å‡†æ¨¡å¼ - åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯")


async def compare_debug_modes():
    """å¯¹æ¯”debugæ¨¡å¼å¼€å¯å’Œå…³é—­çš„æ•ˆæœ"""
    
    print("=" * 80)
    print("ğŸ” LLMAPIClient Debugæ¨¡å¼å¯¹æ¯”ç¤ºä¾‹")
    print("=" * 80)
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç®€æ´åœ°å›ç­”é—®é¢˜ã€‚"},
        {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}
    ]
    
    print("\nğŸš« === æ ‡å‡†æ¨¡å¼ï¼ˆdebug=Falseï¼‰===")
    setup_logging(debug=False)
    
    # åˆ›å»ºæ ‡å‡†æ¨¡å¼å®¢æˆ·ç«¯
    normal_client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=False)
    
    try:
        print("\nğŸ“¤ è°ƒç”¨API...")
        response1 = await normal_client.generate(test_messages, max_tokens=100, temperature=0.5)
        print(f"\nâœ… å“åº”: {response1}")
        
    finally:
        await normal_client.close()
    
    print("\n" + "="*80)
    print("\nğŸ› === Debugæ¨¡å¼ï¼ˆdebug=Trueï¼‰===")
    setup_logging(debug=True)
    
    # åˆ›å»ºdebugæ¨¡å¼å®¢æˆ·ç«¯
    debug_client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        print("\nğŸ“¤ è°ƒç”¨API...")
        response2 = await debug_client.generate(test_messages, max_tokens=100, temperature=0.5)
        print(f"\nâœ… å“åº”: {response2}")
        
    finally:
        await debug_client.close()


async def debug_with_long_conversation():
    """ä½¿ç”¨debugæ¨¡å¼å¤„ç†é•¿å¯¹è¯"""
    
    print("\n" + "="*80)
    print("ğŸ“š Debugæ¨¡å¼ - é•¿å¯¹è¯ç¤ºä¾‹")
    print("="*80)
    
    setup_logging(debug=True)
    
    # åˆ›å»ºdebugå®¢æˆ·ç«¯
    client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        # æ„å»ºä¸€ä¸ªå¤šè½®å¯¹è¯
        conversation = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹ä¸“å®¶ã€‚"},
            {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹ Pythonåˆ—è¡¨æ¨å¯¼å¼ã€‚"},
            {"role": "assistant", "content": "åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­åˆ›å»ºåˆ—è¡¨çš„ç®€æ´æ–¹å¼ã€‚åŸºæœ¬è¯­æ³•æ˜¯ï¼š[expression for item in iterable if condition]"},
            {"role": "user", "content": "èƒ½ç»™æˆ‘ä¸€ä¸ªå…·ä½“çš„ä¾‹å­å—ï¼Ÿ"},
        ]
        
        print("\nğŸ¯ å‘é€å¤šè½®å¯¹è¯...")
        response = await client.generate(conversation, max_tokens=150, temperature=0.3)
        print(f"\nâœ… æœ€ç»ˆå“åº”: {response}")
        
    finally:
        await client.close()


async def debug_with_different_parameters():
    """ä½¿ç”¨debugæ¨¡å¼æµ‹è¯•ä¸åŒå‚æ•°"""
    
    print("\n" + "="*80)
    print("âš™ï¸  Debugæ¨¡å¼ - ä¸åŒå‚æ•°æµ‹è¯•")
    print("="*80)
    
    setup_logging(debug=True)
    
    client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        messages = [{"role": "user", "content": "ç”¨ä¸‰ä¸ªè¯æè¿°æ˜¥å¤©"}]
        
        # æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
        params_list = [
            {"max_tokens": 20, "temperature": 0.1},
            {"max_tokens": 50, "temperature": 0.7},
            {"max_tokens": 80, "temperature": 1.0},
        ]
        
        for i, params in enumerate(params_list, 1):
            print(f"\nğŸ§ª æµ‹è¯• {i}: max_tokens={params['max_tokens']}, temperature={params['temperature']}")
            response = await client.generate(messages, **params)
            print(f"ğŸ“ ç»“æœ: {response}")
            
    finally:
        await client.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("""
ğŸ” LLMAPIClient Debugæ¨¡å¼ä½¿ç”¨è¯´æ˜:

Debugæ¨¡å¼çš„ä½œç”¨ï¼š
1. ğŸ“‹ è®°å½•æ¯æ¬¡APIè¯·æ±‚çš„è¯¦ç»†å‚æ•°
2. ğŸ“¤ æ˜¾ç¤ºå‘é€ç»™LLMçš„å®Œæ•´æ¶ˆæ¯å†…å®¹  
3. ğŸ“¥ è®°å½•LLMè¿”å›çš„å®Œæ•´å“åº”å†…å®¹
4. ğŸ“Š æ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µå’Œå“åº”å…ƒæ•°æ®
5. ğŸ”§ å¸®åŠ©è°ƒè¯•APIè°ƒç”¨é—®é¢˜

ä½¿ç”¨æ–¹æ³•ï¼š
```python
# å¯ç”¨debugæ¨¡å¼
client = create_llm_client(api_key, base_url, model, debug=True)

# æˆ–ä½¿ç”¨å·¥å‚å‡½æ•°
client = create_llm_client(api_key, base_url, model, debug=True)
```

æ³¨æ„ï¼šdebugæ¨¡å¼ä¼šäº§ç”Ÿå¤§é‡æ—¥å¿—ï¼Œå»ºè®®åªåœ¨å¼€å‘å’Œè°ƒè¯•æ—¶ä½¿ç”¨ã€‚
    """)
    
    try:
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        await compare_debug_modes()
        await debug_with_long_conversation()
        await debug_with_different_parameters()
        
        print("\n" + "="*80)
        print("ğŸ‰ Debugæ¨¡å¼æ¼”ç¤ºå®Œæˆï¼")
        print("""
ğŸ’¡ Debugæ¨¡å¼çš„ä¼˜åŠ¿ï¼š
- å®Œæ•´è®°å½•APIäº¤äº’è¿‡ç¨‹
- ä¾¿äºæ’æŸ¥APIè°ƒç”¨é—®é¢˜
- ç›‘æ§tokenä½¿ç”¨æƒ…å†µ
- åˆ†æå“åº”è´¨é‡å’Œå»¶è¿Ÿ

âš ï¸  ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­debugæ¨¡å¼ä»¥å‡å°‘æ—¥å¿—é‡ã€‚
        """)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import openai
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…openaiåº“: pip install openai")
        sys.exit(1)
    
    print("ğŸ” LLMAPIClient Debugæ¨¡å¼æ¼”ç¤º")
    print("è¯¥ç¤ºä¾‹å°†å±•ç¤ºdebugæ¨¡å¼ä¸‹çš„è¯¦ç»†APIè°ƒç”¨æ—¥å¿—")
    
    asyncio.run(main())
