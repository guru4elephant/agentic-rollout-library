#!/usr/bin/env python3
"""
æµ‹è¯•LLMAPIClientçš„åŠŸèƒ½
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨é€šç”¨LLMå®¢æˆ·ç«¯
"""

import asyncio
import sys
import os

# Add the parent directory to the path so we can import workers module
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from workers.utils import create_llm_client


async def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•LLMAPIClientåŸºæœ¬åŠŸèƒ½ ===")
    
    # APIé…ç½®
    api_key = "sk-qq7xJtnAdB1Gv6IkHTQhDAPuUAT700vF3CMmGinILsmP2HuY"
    base_url = "http://211.23.3.237:27544"
    model = "claude-sonnet-4-20250514"
    
    # 1. ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºå®¢æˆ·ç«¯
    print("\n1. ä½¿ç”¨create_llm_clientå·¥å‚å‡½æ•°...")
    client1 = create_llm_client(
        api_key=api_key,
        base_url=base_url,
        model=model,
        debug=False,
        max_retries=2
    )
    
    # 2. åˆ›å»ºç¬¬äºŒä¸ªå®¢æˆ·ç«¯å®ä¾‹
    print("2. åˆ›å»ºç¬¬äºŒä¸ªå®¢æˆ·ç«¯å®ä¾‹...")
    client2 = create_llm_client(
        api_key=api_key,
        base_url=base_url,
        model=model,
        debug=True,
        max_retries=1
    )
    
    try:
        # 3. æµ‹è¯•è¿æ¥
        print("\n3. æµ‹è¯•APIè¿æ¥...")
        try:
            test_messages = [{"role": "user", "content": "Hello"}]
            response = await client1.generate(test_messages, max_tokens=10)
            print(f"è¿æ¥çŠ¶æ€: âœ… æˆåŠŸ (å“åº”: {response[:30]}...)")
        except Exception as e:
            print(f"è¿æ¥çŠ¶æ€: âŒ å¤±è´¥ ({e})")
        
        # 4. æµ‹è¯•ç®€å•å¯¹è¯
        print("\n4. æµ‹è¯•ç®€å•å¯¹è¯...")
        messages = [
            {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯ReActæ¡†æ¶"}
        ]
        
        response = await client1.generate(messages, max_tokens=200, temperature=0.3)
        print(f"å“åº”: {response[:200]}...")
        
        # 5. æµ‹è¯•å¤šè½®å¯¹è¯
        print("\n5. æµ‹è¯•å¤šè½®å¯¹è¯...")
        conversation = [
            {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½"},
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘å¾ˆä¹æ„ä¸ºæ‚¨ä»‹ç»äººå·¥æ™ºèƒ½ã€‚"},
            {"role": "user", "content": "è¯·ç®€å•è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ "}
        ]
        
        response = await client2.generate(conversation, max_tokens=150, temperature=0.5)
        print(f"å¤šè½®å¯¹è¯å“åº”: {response[:150]}...")
        
    finally:
        # æ¸…ç†èµ„æº
        await client1.close()
        await client2.close()


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\n=== æµ‹è¯•é”™è¯¯å¤„ç† ===")
    
    # æµ‹è¯•é”™è¯¯çš„APIé…ç½®
    print("\n1. æµ‹è¯•é”™è¯¯çš„APIé…ç½®...")
    try:
        bad_client = create_llm_client(
            api_key="invalid-key",
            base_url="http://invalid-url",
            model="invalid-model",
            max_retries=1
        )
        response = await bad_client.generate([{"role": "user", "content": "test"}], max_tokens=10)
        print(f"é”™è¯¯å¤„ç†å“åº”: {response[:100]}...")
        await bad_client.close()
    except Exception as e:
        print(f"æ•è·åˆ°å¼‚å¸¸: {e}")


async def test_different_parameters():
    """æµ‹è¯•ä¸åŒå‚æ•°é…ç½®"""
    print("\n=== æµ‹è¯•ä¸åŒå‚æ•°é…ç½® ===")
    
    api_key = "sk-qq7xJtnAdB1Gv6IkHTQhDAPuUAT700vF3CMmGinILsmP2HuY"
    base_url = "http://211.23.3.237:27544"
    model = "claude-sonnet-4-20250514"
    
    client = create_llm_client(api_key, base_url, model)
    
    try:
        # 1. æµ‹è¯•ä¸åŒæ¸©åº¦
        print("\n1. æµ‹è¯•ä¸åŒæ¸©åº¦å‚æ•°...")
        messages = [{"role": "user", "content": "ç”¨ä¸€å¥è¯æè¿°æ˜¥å¤©"}]
        
        for temp in [0.1, 0.7, 1.0]:
            response = await client.generate(messages, max_tokens=50, temperature=temp)
            print(f"æ¸©åº¦ {temp}: {response[:80]}...")
        
        # 2. æµ‹è¯•ä¸åŒtokené™åˆ¶
        print("\n2. æµ‹è¯•ä¸åŒtokené™åˆ¶...")
        messages = [{"role": "user", "content": "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"}]
        
        for tokens in [50, 100, 200]:
            response = await client.generate(messages, max_tokens=tokens, temperature=0.5)
            print(f"æœ€å¤§tokens {tokens}: {response[:100]}...")
            
    finally:
        await client.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("LLMAPIClient åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    try:
        await test_basic_functionality()
        await test_error_handling() 
        await test_different_parameters()
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("""
    ğŸ§ª LLMå®¢æˆ·ç«¯æµ‹è¯•è¯´æ˜:
    
    æœ¬æµ‹è¯•å°†éªŒè¯LLMAPIClientçš„ä»¥ä¸‹åŠŸèƒ½ï¼š
    1. åŸºæœ¬å®ä¾‹åŒ–å’Œå·¥å‚å‡½æ•°åˆ›å»º
    2. APIè¿æ¥æµ‹è¯•
    3. ç®€å•å¯¹è¯ç”Ÿæˆ
    4. å¤šè½®å¯¹è¯æ”¯æŒ
    5. é”™è¯¯å¤„ç†æœºåˆ¶
    6. ä¸åŒå‚æ•°é…ç½®
    
    æµ‹è¯•ä½¿ç”¨çš„APIç«¯ç‚¹:
    - Base URL: http://211.23.3.237:27544/
    - Model: claude-sonnet-4-20250514
    """)
    
    asyncio.run(main())