#!/usr/bin/env python3
"""
Enhanced Debugæ¨¡å¼ç¤ºä¾‹
å±•ç¤ºæ”¹è¿›åçš„debugæ¨¡å¼å¦‚ä½•å®Œæ•´æ˜¾ç¤ºè¾“å…¥å’Œè¾“å‡ºä¿¡æ¯
"""

import asyncio
import logging
import sys
import os

# Add the parent directory to the path so we can import workers module
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from workers.utils import create_llm_client

# APIé…ç½®
API_KEY = os.getenv("LLM_API_KEY", "your-api-key-here")
BASE_URL = os.getenv("LLM_BASE_URL", "your-base-url-here")
MODEL_NAME = os.getenv("LLM_MODEL_NAME", "claude-3-sonnet")


def setup_debug_logging():
    """è®¾ç½®debugçº§åˆ«çš„æ—¥å¿—"""
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # è®¾ç½®debugçº§åˆ«æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S',
        force=True
    )
    
    # ç¡®ä¿LLMå®¢æˆ·ç«¯çš„æ—¥å¿—ä¹Ÿæ˜¯DEBUGçº§åˆ«
    logging.getLogger('workers.utils.llm_client').setLevel(logging.DEBUG)


async def test_short_input():
    """æµ‹è¯•çŸ­è¾“å…¥çš„debugæ˜¾ç¤º"""
    print("\nğŸ” === æµ‹è¯•çŸ­è¾“å…¥çš„Debugæ˜¾ç¤º ===")
    
    client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        messages = [
            {"role": "user", "content": "Hello, world!"}
        ]
        
        print("ğŸ“¤ å‘é€çŸ­æ¶ˆæ¯...")
        response = await client.generate(messages, max_tokens=50, temperature=0.5)
        print(f"âœ… æ”¶åˆ°å“åº”: {response}")
        
    finally:
        await client.close()


async def test_long_input():
    """æµ‹è¯•é•¿è¾“å…¥çš„debugæ˜¾ç¤º"""
    print("\nğŸ” === æµ‹è¯•é•¿è¾“å…¥çš„Debugæ˜¾ç¤º ===")
    
    client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        # æ„é€ ä¸€ä¸ªå¾ˆé•¿çš„è¾“å…¥
        long_content = """
è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„è¾“å…¥æ¶ˆæ¯ï¼Œç”¨æ¥æµ‹è¯•debugæ¨¡å¼å¦‚ä½•å¤„ç†é•¿æ–‡æœ¬ã€‚
æˆ‘ä»¬åœ¨è¿™é‡ŒåŒ…å«äº†å¤§é‡çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. è¯¦ç»†çš„ä»»åŠ¡æè¿°
2. å¤æ‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
3. å¤šä¸ªç¤ºä¾‹å’Œè¯´æ˜
4. æŠ€æœ¯ç»†èŠ‚å’Œè§„èŒƒ
5. æœŸæœ›çš„è¾“å‡ºæ ¼å¼

åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œç”¨æˆ·å¯èƒ½ä¼šå‘é€éå¸¸é•¿çš„promptï¼ŒåŒ…å«ï¼š
- å®Œæ•´çš„æ–‡æ¡£å†…å®¹
- ä»£ç ç‰‡æ®µå’Œç¤ºä¾‹
- è¯¦ç»†çš„æŒ‡ä»¤å’Œè¦æ±‚
- å†å²å¯¹è¯è®°å½•
- å¤æ‚çš„æ•°æ®ç»“æ„

Debugæ¨¡å¼åº”è¯¥èƒ½å¤Ÿå®Œæ•´åœ°æ˜¾ç¤ºè¿™äº›ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒæ—¥å¿—çš„å¯è¯»æ€§ã€‚
é•¿æ–‡æœ¬åº”è¯¥æ—¢æ˜¾ç¤ºé¢„è§ˆç‰ˆæœ¬ï¼ˆå‰å‡ ç™¾ä¸ªå­—ç¬¦ï¼‰ï¼Œä¹Ÿæ˜¾ç¤ºå®Œæ•´ç‰ˆæœ¬ï¼Œ
è¿™æ ·å¼€å‘è€…å¯ä»¥æ ¹æ®éœ€è¦æŸ¥çœ‹æ‰€éœ€çš„è¯¦ç»†ç¨‹åº¦ã€‚

è¿™ä¸ªæµ‹è¯•ç”¨ä¾‹å°±æ˜¯ä¸ºäº†éªŒè¯è¿™ç§é•¿æ–‡æœ¬çš„å¤„ç†èƒ½åŠ›ã€‚
""".strip()
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¤„ç†é•¿æ–‡æœ¬åˆ†æä»»åŠ¡ã€‚"},
            {"role": "user", "content": long_content},
            {"role": "user", "content": "è¯·æ€»ç»“ä¸Šè¿°å†…å®¹çš„è¦ç‚¹ã€‚"}
        ]
        
        print("ğŸ“¤ å‘é€é•¿æ¶ˆæ¯...")
        response = await client.generate(messages, max_tokens=200, temperature=0.3)
        print(f"âœ… æ”¶åˆ°å“åº”: {response}")
        
    finally:
        await client.close()


async def test_multi_turn_conversation():
    """æµ‹è¯•å¤šè½®å¯¹è¯çš„debugæ˜¾ç¤º"""
    print("\nğŸ” === æµ‹è¯•å¤šè½®å¯¹è¯çš„Debugæ˜¾ç¤º ===")
    
    client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        # æ„å»ºä¸€ä¸ªå¤æ‚çš„å¤šè½®å¯¹è¯
        conversation = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹ä¸“å®¶å’Œæ•™å­¦åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹ Pythonçš„è£…é¥°å™¨ï¼Œèƒ½ç»™æˆ‘è§£é‡Šä¸€ä¸‹å—ï¼Ÿ"},
            {"role": "assistant", "content": "å½“ç„¶ï¼è£…é¥°å™¨æ˜¯Pythonä¸­çš„ä¸€ä¸ªå¼ºå¤§ç‰¹æ€§ã€‚ç®€å•æ¥è¯´ï¼Œè£…é¥°å™¨æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—å¦ä¸€ä¸ªå‡½æ•°ä½œä¸ºå‚æ•°ï¼Œå¹¶æ‰©å±•æˆ–ä¿®æ”¹å…¶è¡Œä¸ºï¼Œè€Œä¸æ˜¾å¼ä¿®æ”¹å‡½æ•°æœ¬èº«ã€‚"},
            {"role": "user", "content": "èƒ½ç»™æˆ‘ä¸€ä¸ªå…·ä½“çš„ä¾‹å­å—ï¼Ÿæ¯”å¦‚è®¡æ—¶è£…é¥°å™¨ï¼Ÿ"},
            {"role": "assistant", "content": "å¥½çš„ï¼è¿™æ˜¯ä¸€ä¸ªè®¡æ—¶è£…é¥°å™¨çš„ä¾‹å­ï¼š\n\n```python\nimport time\nimport functools\n\ndef timer(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(f'{func.__name__} took {end - start:.4f} seconds')\n        return result\n    return wrapper\n\n@timer\ndef slow_function():\n    time.sleep(1)\n    return 'Done'\n```"},
            {"role": "user", "content": "è¿™ä¸ªä¾‹å­å¾ˆå¥½ï¼ä½†æ˜¯æˆ‘è¿˜æƒ³äº†è§£å¸¦å‚æ•°çš„è£…é¥°å™¨ï¼Œèƒ½å†ç»™æˆ‘ä¸€ä¸ªä¾‹å­å—ï¼Ÿ"}
        ]
        
        print("ğŸ“¤ å‘é€å¤šè½®å¯¹è¯...")
        response = await client.generate(conversation, max_tokens=300, temperature=0.4)
        print(f"âœ… æ”¶åˆ°å“åº”: {response}")
        
    finally:
        await client.close()


async def test_different_parameters():
    """æµ‹è¯•ä¸åŒå‚æ•°çš„debugæ˜¾ç¤º"""
    print("\nğŸ” === æµ‹è¯•ä¸åŒå‚æ•°çš„Debugæ˜¾ç¤º ===")
    
    client = create_llm_client(API_KEY, BASE_URL, MODEL_NAME, debug=True)
    
    try:
        messages = [{"role": "user", "content": "è¯·ç”Ÿæˆä¸€ä¸ªåˆ›æ„çš„æ•…äº‹å¼€å¤´ã€‚"}]
        
        # æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ
        test_cases = [
            {"max_tokens": 30, "temperature": 0.1, "description": "ä½åˆ›é€ æ€§ï¼ŒçŸ­è¾“å‡º"},
            {"max_tokens": 100, "temperature": 0.8, "description": "é«˜åˆ›é€ æ€§ï¼Œä¸­ç­‰è¾“å‡º"},
            {"max_tokens": 200, "temperature": 1.0, "description": "æœ€é«˜åˆ›é€ æ€§ï¼Œé•¿è¾“å‡º"},
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ§ª æµ‹è¯• {i}: {test_case['description']}")
            print(f"   å‚æ•°: max_tokens={test_case['max_tokens']}, temperature={test_case['temperature']}")
            
            response = await client.generate(
                messages, 
                max_tokens=test_case['max_tokens'], 
                temperature=test_case['temperature']
            )
            print(f"ğŸ“ ç»“æœ: {response}")
            
    finally:
        await client.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Enhanced Debugæ¨¡å¼æµ‹è¯•")
    print("=" * 80)
    print("""
ğŸ“‹ æœ¬æµ‹è¯•å°†éªŒè¯å¢å¼ºçš„debugæ¨¡å¼åŠŸèƒ½ï¼š

1. âœ… å®Œæ•´æ˜¾ç¤ºè¾“å…¥ä¿¡æ¯
   - æ¶ˆæ¯çš„roleå’Œcontent
   - å†…å®¹é•¿åº¦ç»Ÿè®¡
   - é•¿æ–‡æœ¬çš„é¢„è§ˆå’Œå®Œæ•´æ˜¾ç¤º

2. âœ… è¯¦ç»†æ˜¾ç¤ºAPIå‚æ•°
   - æ¨¡å‹åç§°
   - max_tokenså’Œtemperature
   - è¯·æ±‚å…ƒæ•°æ®

3. âœ… å®Œæ•´æ˜¾ç¤ºè¾“å‡ºä¿¡æ¯
   - Tokenä½¿ç”¨ç»Ÿè®¡
   - å“åº”å†…å®¹é•¿åº¦
   - å®Œæ•´å“åº”å†…å®¹

4. âœ… æ¸…æ™°çš„æ ¼å¼åŒ–æ˜¾ç¤º
   - ä½¿ç”¨åˆ†éš”çº¿å’Œemoji
   - ç»“æ„åŒ–çš„ä¿¡æ¯å¸ƒå±€
   - æ˜“äºé˜…è¯»çš„æ ¼å¼
    """)
    
    # è®¾ç½®debugæ—¥å¿—
    setup_debug_logging()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        await test_short_input()
        await test_long_input() 
        await test_multi_turn_conversation()
        await test_different_parameters()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ Enhanced Debugæ¨¡å¼æµ‹è¯•å®Œæˆï¼")
        print("""
ğŸ’¡ Debugæ¨¡å¼çš„æ”¹è¿›ï¼š
âœ… è¾“å…¥ä¿¡æ¯å®Œæ•´æ˜¾ç¤º - åŒ…æ‹¬æ¯æ¡æ¶ˆæ¯çš„roleã€contentå’Œé•¿åº¦
âœ… é•¿æ–‡æœ¬æ™ºèƒ½å¤„ç† - æ—¢æ˜¾ç¤ºé¢„è§ˆä¹Ÿæ˜¾ç¤ºå®Œæ•´å†…å®¹
âœ… ç»“æ„åŒ–æ—¥å¿—æ ¼å¼ - ä½¿ç”¨åˆ†éš”çº¿å’Œemojiä¾¿äºé˜…è¯»
âœ… è¯¦ç»†çš„Tokenç»Ÿè®¡ - ç²¾ç¡®çš„ä½¿ç”¨æƒ…å†µåˆ†æ
âœ… å“åº”å†…å®¹å®Œæ•´è®°å½• - æ”¯æŒé•¿å“åº”çš„å®Œæ•´æ˜¾ç¤º

ğŸ”§ é€‚ç”¨åœºæ™¯ï¼š
- APIè°ƒç”¨é—®é¢˜æ’æŸ¥
- Tokenä½¿ç”¨ä¼˜åŒ–åˆ†æ
- é•¿æ–‡æœ¬å¤„ç†éªŒè¯
- å¤šè½®å¯¹è¯è°ƒè¯•
- å‚æ•°æ•ˆæœå¯¹æ¯”
        """)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import openai
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…openaiåº“: pip install openai")
        sys.exit(1)
    
    print("ğŸ” Enhanced Debugæ¨¡å¼ - å®Œæ•´è¾“å…¥è¾“å‡ºæ˜¾ç¤ºæµ‹è¯•")
    print("è¯¥ç¤ºä¾‹å°†å±•ç¤ºæ”¹è¿›åçš„debugæ¨¡å¼å¦‚ä½•å®Œæ•´æ˜¾ç¤ºæ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºä¿¡æ¯")
    
    asyncio.run(main())